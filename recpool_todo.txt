Is decoding_pooling_dictionary training?  It doesn't appear to change.  encoding_pooling_dictionary trains, but this may be due solely to the classification loss.
Why is pooling L2 position loss so small?
Take unequal lambdas into account when calculating position units and pooling reconstruction loss.  This may allow us to turn up the pooling reconstruction loss without forcing everything to zero; as it is, reconstructions are smaller than they should be, so pooling reconstruction errors can be reduced by ensuring that the shrink layer is all-zero.

Pooling reconstruction is still very small!

Why is the last element of the pooling dictionary flat?

10 epochs, all lambda = 0, 
95.5%
Forward filters look like hazy digits and digit-halves.  Perhaps 2/3 are strongly trained.

10 epochs, feature encoding lambdas = 0
59.6%
Only 12 forward feature extracting filters train significantly, and look like average digits.  The number of significantly trained filters increases with training.  The sparse representation only has at most one non-zero element.  Presumably, this makes the pooling units sparse and increases the accuracy of the pooling reconstruction.  The encoding and decoding pooling dictionaries train, but not drastically.


10 epochs, pooling lambdas = 0, 2e-3 reconstruction sparsity
95.2%
All encoding filters train, perhaps half strongly and tend to look like very hazy digits.  Shrink representations are not very sparse (at most half zeros, which would probably be expected from using non-negative units alone).  Decoding filters have structure, but it is all small scale, and they are extremely noisy on the whole, suggesting that sparsification is insufficient without the pooling lambdas.  Encoding pooling dictionaries become less sparse.

10 epochs, pooling lambdas = 0, 5e-2 reconstruction sparsity
95.8%
Almost all decoding filters seem to be moving towards the representation of digit parts; few represent whole digits.  Encoding filters are suprisingly clear, especially those that correspond to digit part decodings, rather than full digits.  Encoding pooling dictionaries learn to be less sparse.  Shrink representations are perhaps 80% sparse.


10 epochs, all lambdas used except pooling sparsity = 0, 1e-2 reconstruction sparsity, 1e-2 mask sparsity
95.7%
Extreme sparsity observed when using full pooling-layer regularization seems to be due to the large coefficient used on the pooling sparsity
Shrink representations are perhaps 60% sparse (i.e., mostly unsparsened, given non-negative units).  Decoding pooling dictionary is not visibly trained; encoding pooling dictionary becomes a little less sparse, but not as much as without mask sparsity.  Decoding filters only have high frequency structure, and so are obviously insufficiently sparsened.  About 1/3 of encoding feature extracting filters are strongly trained and look like hazy digits  


10 epochs, no lambdas set to zero, all sparsifying lambdas 5e-2
95.2%
Both encoding and decoding feature extraction dictionaries look like full digits, although there are some digit parts.  Shrink representation is perhaps 90% sparse.  In contrast to unregularized approach, classification error reduces evenly throughout training, rather than quickly obtaining 90% correct and then improving very slowly.  Encoding pooling dictionaries are not rendered overly non-sparse; decoding pooling dictionaries change a little.


10 epochs, all lambdas used but pooling sparsity = 0, 5e-2 reconstruction sparsity, 5e-2 mask sparsity
94.5%
Shrink representations are perhaps 90% sparse.  There are about 10 elements of the decoding feature extraction dictionary that consist of smooth digits; the rest seem to be evolving towards digit parts.  At least half of the elements of the encoding feature extraction dictionary have small weights.  Remember that mask and pooling sparsity have very similar effects; consider using half as much for each as for feature encoding sparsity.  Classification performance improves consistently throughout the 10 epochs.  Encoding pooling dictionaries are not rendered overly non-sparse; decoding pooling dictionaries change a little.


10 epochs, all lambdas but L2_position_unit = 0, 5e-2 feature reconstruction sparsity, 2.5e-2 pooling and mask sparsity
96.6%
Shrink representations are about 80% sparse.  All trained elements of the decoding feature extraction dictionary are feature parts.  Encoding pooling dictionaries are not rendered overly non-sparse, but are perhaps a little less sparse than with other lambda settings; decoding pooling dictionaries change a little.


10 epochs, all lambdas but L2_position_unit = 1, 5e-2 feature reconstruction sparsity, 2.5e-2 pooling and mask sparsity
94.6%
Pooling position units are an order of magnitude smaller.  Remember that L1 and L2 scaling factors should be matched to induce pooling behavior when directly minimizing the bilinear reconstruction loss; presumably the L2_position_unit lambda should match the sum of the pooling and mask sparsities.  Shrink representations are about 90% sparse.  Trained elements of the decoding feature extraction dictionary look like whole digits or differences between digit pairs.  The decoding pooling dictionary is rendered noticeably non-sparse, in contrast to previous runs.


10 epochs, all lambdas; 5e-2 feature reconstruction sparsity and L2_position_unit, 2.5e-2 pooling and mask sparsity
95.2% (98.4% after 20 epochs)
Shrink representations are about 85% sparse.  Almost all trained elements of the decoding feature extraction dictionary are feature parts.  Encoding pooling dictionaries are not rendered overly non-sparse; decoding pooling dictionaries change a little.  There's probably too much sparsity within pooling groups at the feature extraction level.  While there is some similarlity between nearby feature extraction units, such units are often not coactive.  We would like sparsity to be enforced more at the pooling level than at the feature extraction level.  Note that the 5e-2 lambda seems optimal without any pooling regularization; try turning down the feature-extraction-level sparsity.


10 epochs 3e-2 feature reconstruction sparsity, 1.5e-2 pooling and mask sparsity, 3e-2 L2_position_unit
95.6% (98.8% after 20 epochs)
Shrink representations are perhaps 80% sparse.  Decoding feature extraction dictionary elements are all feature parts, although disturbingly some remain mostly untrained even after 26 epochs.  Some elements of the decoding pooling dictionary become very nonsparse (although with small magnitude).  THere's still probably too much sparsity within pooling groups at the feature extraction level, and not enough between groups at the pooling level.  

10 epochs 1.5e-2 feature reconstruction sparsity, 3e-2 pooling and mask sparsity, 6e-2 L2_position_unit
95.3% (98.96% after 20 epochs)
Many feature extraction units are still underused.

10 epochs 0.5e-2 feature reconstruction sparsity, 4e-2 pooling and mask sparsity, 8e-2 L2_position_unit
95.96% (98.96% after 20 epochs)

10 epochs 4 feature reconstruction L2 error, 1 pooling reconstruction L2 error, 0.5e-2 feature reconstruction sparsity, 4e-2 pooling and mask sparsity, 8e-2 L2_position_unit
96.54% (99.26% after 20 epochs)

10 epochs 4 feature reconstruction L2 error, 1 pooling reconstruction L2 error, 2e-2 feature reconstruction sparsity, 4e-2 pooling and mask sparsity, 8e-2 L2_position_unit
96.42% (99.34% after 20 epochs)
Sparsity seems close to 70% - probably need to make sparser

10 epochs 4 feature reconstruction L2 error, 1 pooling reconstruction L2 error, 4e-2 feature reconstruction sparsity, 4e-2 pooling and mask sparsity, 8e-2 L2_position_unit
96.88% (99.22% after 20 epochs)
Sparsity seems about 80%




Try doing one or two epochs of 50,000 elements.  Compare lambdas to no regularization.
Consider including a full reconstruction of the input from the pooling units, passing through the decoding feature extraction dictionary
Why are encoding feature extraction dictionary elements so noisy?  Could be that the gradients are too large.  Remember that the gradients on the encoding and explaining away dictionaries are larger than on the decoding dictionaries (CHECK)


CONFIRM that without the reconstruction error, traininable pooling fails, since the sparsity term causes renders most units inactive (is this still true when there is a supervised loss?).  Without reconstruction error, deep networks can't be trained in an unsupervised fashion.  Confirm that trainable pooling (with reconstruction error) outperforms nontrainable pooling with 50,000 MNIST datapoints.

20,000 datapoints, 7 epochs, two-sided normalization of decoding columns, no pooling reconstruction, random pooling initialization
96.025%
95.14% with pooling reconstruction
95.55% with 1 orig/0.25 shrink pooling reconstruction - pooling output is significantly sparsified, but pooling reconstruction is not very sparse at all.


Where do nans come from when we turn up the mask sparsity!?! - nn.Power and nn.CMulTable produced nans in gradInput when the input is exactly equal to zero

CAUCHY COST DOES NOT SEEM TO BEHAVE SENSIBLY FOR SMALL VALUES!  IT IS NOT SPARSIFYING FOR SMALL x!!!  Working on adding a constant multiplier.  Changed mask to L1
Why does a mask L1 norm have such a large effect?  
Check relative magnitude of gradients on different parameter matrices.  Adjust so they are more even!  decoding_pooling_dictionary has gradient norm 10% as large as the encoding_feature_extracting_dictionary.  Try scaling encoding-pooling by 2 and decoding-pooling by 10

If L1 and L2 lambdas aren't equal, how does that affect the optimality of traditional pooling?  Actually, it seems to make traditional pooling even better, especially with L1 regularization, since the additive constant in the denominator is smaller.  However, this also implies that the position units are almost exactly the terms required to get perfect reconstruction, so even though the scaling factor on the pooling reconstruction is large, the actual reconstruction error will generally be small.  The ratio that matters should be that between L2 reconstruction and L2 pooling, since that sets the position values.  A smaller L1 regularizer should just make the optimal pooling output smaller. 

Why are decoding dictionary elements so poor?  Try allowing the network to train much longer (doesn't seem to help).  Check the quality of the parts learned when pooling reconstruction error and sparsity is turned off completely; confirm that the separation of encoder and decoder is not an insuperable problem.  Results from 9/3 suggest that, when sparsity and reconstruction errors are only imposed on the feature extraction layer, sensible-looking parts are learned.  Consider ensuring that the additional regularization on pooling is sufficiently weak that it does not disrupt this process.  Note that the encoding pooling dictionaries learned when only the feature extraction is regularized are already sparse, and tend to have only one strong connection.  A rerun with the current set of parameters (and with the pooling dictionary initialized randomly) seems much messier.  Note that the 9/3 results use much less sparsity (5e-3), but due use pooling regularization, and learn strings of similar consecutive feature extraction filters.



Check again what happens if we turn the pooling reconstruction regularizers off.  Make the mask sparsity a ParameterizedL1Cost.  Do variable L1 scaling factors properly find the best reconstruction with a given level of sparsity, given that the degree of regularization changes as the sparsity changes?

Imposing L1 regularizers on the pooling output or mask without a pooling reconstruction error doesn't make sense, since the encoding pooling dictionary can just shrink to nothing as the classification dictionary grows to compensate; it's magnitude is not propped up by any term of the loss function.  As a result, the set point for the pooling sparsity can be anomalously low, since it can be reduced by just scaling down the encoding pooling dictionary and scaling up the classification dictionary.

It's odd to control the L2 norm of the decoding pooling dictionary, the L1 mask norm, and the L1 pooling norm.  It seems important to control the L1 norm of the feature extraction units, since the shrink operation can easily turn units off, and the approximate ISTA operation can cause some units to be very active in response to a disproportionate percentage of the inputs.  The pooling units, in contrast, will generally not be silenced entirely.  Insufficient mask activity should be handled by the combination of L1 regulation of the feature extraction units and the L2 pooling reconstruction loss.  

The position units and thus the actual reconstruction only have a sensible magnitude if the mask values are close to 1.  Otherwise, the position units will be small, so the reconstruction will be small, and the reconstruction error will be largely irrelevant, since it will be due primarily to the small magnitude of the reconstruction, and will not be substantially affected by the quality of the reconstruction.

Rather than find the optimal scaling of the reconstruction pooling regularizer, show that a small increment from zero improves performance.  A full search can be performed later.


Is initial encoder pooling dictionary large enough?
Check facial plausibility of pooling reconstruction operation.  
Check that mismatch between L1 and L2 penalties is not problematic in the context of the decoder only (minimize the energy, rather than use the encoder).  When the L1 term is very small, this implies that the minimum of the decoding loss has huge pooling units relative to the position units.  Indeed, in practice the position units are about 1e-2 as large as the pooling units, exactly matching the ratio between the scaling factors on the L1 and L2 loss functions.  HOWEVER, the scaling induced by the 



Why does the magnitude decay as we pass through the layers?  This implies that sparsity will set activity to zero, and pooling reconstruction error will be dwarfed by the constant.  Track the change in L2 norm of the representations as we move through the hierarchy.  Solved by adding divisive normalization to each recpool layer.

Four hidden layers seems to be sufficient to get bad performance without reconstruction loss on the pooling layer.  With only three hidden layers, the magnitude of the gradients on the first layer encoding pooling dictionary are less than 10% of those on the other encoding pooling dictionaries, but training is still effective.  The encoding feature extraction dictionaries, in contrast, all have gradients with similar magnitudes, presumably because these gradients are largely driven by the feature extraction reconstruction error, which I leave in.  89.72% after 15 epochs of 5000 with three hidden layers.  Learning slows down considerably as it progresses.  Does the same thing happen with reconstruction pooling?  For unclear reasons, reconsturction error slows learning considerably, and only acheives about 60% after 15 epochs of 5000 with three hidden layers.

Why is the decoding feature extraction dictionary so poor?  Is there insufficient sparsity in the first layer, leading to bad features?  The problem seems to be due to the low reconstruction loss scaling AND the low sparsity used on the first layer.  With two recpool layers, we can get good features if we turn up the reconstruction loss for both layers, and make the sparsity loss for the first layer much larger than that in the second layer (comparable to that used with only one recpool layer)

Pooling reconstruction doesn't seem to be doing anything for the first of two hidden layers.  Second layer decoding pooling dictionary starts out non-sparse (it's unclear if the encoding dictionary learns to be sparse, or the decoding dictionary learns to be non-sparse in the first epoch), but grows sparse with trainingw.  Pooling output from first layer is not sparse at all, which likely explains the poor pooling reconstruction, since roughly the same reconstruction is activated on all iterations.  Some sparsity develops in the pooling output from the first layer when it is trained alone, with classification turned off, so long as the pooling weights are initialized to be contigous groups.

Gradients on the encoding pooling dictionary can grow monstrously huge, but it doesn't seem to change (graphically) relative to the decoding pooling dictionary, which consistently has tiny gradients.  What's going on?  This is further surprising since no criterion seems to be producing a huge gradient.  There are a small number of very large values.  The problem was that after I reduced the additive constant fed into the square root, the gradient of the square root exploded.  I've attempted to resolve the problem by adding in a relatively large constant, but then subtracting out its square root afterwards, so zero is mapped to zero by the composite operation.

Pooling reconstruction seems to be anticorrelated with the input.  The largest inputs tend to have reconstructions that are tiny or equal to zero.  Keep in mind that the L2 position loss is relevant, and is minimized by exactly this sort of mismatch if alpha is large compared to the magnitude of the reconstructions.  Alpha should thus be around 1e-3

Only a small subset of the pooling units ever appears to be large.

2 layers, reconstruction pooling loss
68.34 after 10 epochs; 94.78 after 20 epochs; 96.8 after 27 epochs
75.28 after 10 epochs; 94.56 after 20 epochs; 96.82 after 30 epochs
2 layers, reconstruction pooling loss, nonnegative explaining away diagonal
76.3 after 10 epochs; 94.54 after 20 epochs; 97.3 after 30 epochs; 98.38 after 40 epochs (no longer improving consistently)
2 layers, only classification loss
89.72 after 10 epochs; 94.7 after 20 epochs; goes to nans shortly thereafter
91.36 after 10 epochs; 95.64 after 20 epochs; 96.96 after 30 epochs
2 layer, 10 epochs of purely unsupervised pretraining
85 after 10 epochs; 96.68 after 20 epochs; 98.52 after 30 epochs (no longer improving consistently)
2 layer, 20 epochs of purely unsupervised pretraining
80.76 after 10 epochs; 94.88 after 20 epochs; 98.42 after 30 epochs (no longer improving consistently) - turning down the training rate from 5e-3 to 5e-4 seems to significantly improve the converged performance, from 98.42 to 99.2 even without classification loss - turning down from 5e-3 to 5e-4 with classification loss improves performance to at least 99.74%, and doesn't seem to be bounded 
Long run on cassio with two hidden layers: 99.838% on the training set of 50,000;  97.13% on validation set of 10,000; 97.15% on test set of 10,000 - decoding_pooling_dictionary_1 often has only one non-zero element, and is thus much too sparse.  Mask sparsity is too strong relative to pooling sparsity.  Although many first-layer feature extraction units aren't used, it's evident that the trained subset is sufficient to solve the training set, but better regularization is necessary to obtain good performance on the validation/testing set.  This might require a different ratio of sparsifying to reconstruction losses, but might also require a different trade-off between regularization and classification losses.  

2 layers, correct normalization, reconstruction pooling loss, rebalanced mask versus pooling L1
90.7 after 10 epochs; 
First layer is too sparse; second layer isn't sparse enough

2 layers, only classification loss; long run
99.6% training set loss (5000 elements) after 121 epochs; 97% validation loss (5000 elements) after 121 epochs

2 layers, correct normalization, reconstruction pooling loss, 50,000 training set elements
99.8% train set loss (5000 elements) after 31 epochs; 97.63% validation loss (10,000 elements) after 31 epochs (97.47 with reduced_feature_extraction_sparsity)
Decoding pooling dictionaries are not sparse at all!!!  Reconstruction of some units seem to be turned off completely, whereas others seem to be almost uniformly active.

Using units that can go negative does not yield obviously better results: 10 epochs over 5000 elements gives an error of 74.46%

Why do the diagonal elements of explaining_away become negative?  I'm now forcibly holding the diagonal elements of explaining_away non-negative.  THEY SEEM TO GO TO ZERO for all (most?) strongly trained filters.

How does pooling_reconstruction become exactly equal to zero?  decoding_pooling_dictionary is non-negative, corresponding to the non-negativity of the feature extraction units.  However, since pooling_output is generally positive, reconstruction values exactly equal to zero imply that all columns of the decoding_pooling_dictionary have zeros in the corresponding entries.


Can we learn weighted L2 divisive normalization based upon reconstruction by optimizing the pooling units and outputting the position units, the opposite of the arrangement used for pooling?  This isn't as simple, since taking the derivative of the loss with respect to the pooling units and setting it equal to zero, the pooling units are subject to nontrivial linear transforms, which would need to be inverted to solve for the pooling units.  However, we can always perform the same calculations as for the pooling layer, but return the position units instead of the pooling units.  The network then optimizes the position units given a learned approximation for the pooling units.  The resulting calculations are similar to traditional divisive normalization, but instead of dividing by the square root of the weighted sum of square, we divide by the weighted sum of the square root of the weighted sum of squares; there's an extra weighted sum, since a small, common set of pooling units is used for all position units.  This becomes equivalent to traditional contrast normalization if the pooling regions are disjoint and all-ones.  
CHECK that pooling dictionaries actually change, just as feature extraction dictionaries change.  Try loading learned feature extraction dictionary and randomizing the pooling dictionary; confirm that pooling dictionaries change significantly, even without the classification loss function.  (After randomizing pooling dictionaries with classification loss, they do not converge back to their original values.  In fact, there seems to be almost no overlap.)
Compare performance with logistic units rather than non-negative ISTA units; compare performance with ISTA units that can be negative
If gamma tones are the sparse features of human speech, and PLP features the topological ICA/reconstruction pooling features of speech?
What does GMM look like as a feedforward network?  Similar to softmax?
Consider the possibility that feature extraction filters go untrained because mask sparsity is too high relative to pooling sparsity, since this might force the decoding pooling dictionary to be consistently zero for some feature extraction units.  On the other hand, maybe the issue is that, in order to push some pooling units to always be zero (which happens in practice), the associated feature extraction units are forced to be zero.  This may be easier than pushing all of the associated encoding pooling dictionary weights to zero.  Decreasing the amount of pooling unit sparsification seems to improve the appearance of the feature extraction dictionary elements, and seems to reduce the prevalence of pooling units that are completely silenced.  Try longer pretraining and reduced sparsity.


Sparsity in pooling output is often achieved by making some pooling units consistently very small (i.e., unused).  Perhaps sufficient to ensure that feature extraction units never actually go to zero.  Try smoothed version used by Karol or Koray! (softshrink)  However, gradients will still be very small, although not exactly equal to zero.  Reintroducing lagrange multipliers on the mask sparsity might be more effective, since it directly addresses the issue.

When using lagrange multipliers on the mask sparsity, pooling output is not very sparse, especially in the first hidden layer.  This may be problematic.  Check that this resolved when lagrange update rate is set to 0.

The final four elements of explaining_away in the filter image lack a diagonal element equal to one, since the 200-element vector is being displayed in a 14*14 = 196 element square.

Consider trying deeper ista stacks.  Three is much too small with units that are allowed to go negative.  It seems plausible that sparsity is being achieved by taking advantage of nonnegativity and setting the diagonal of explaining_away to zero (or negative, if this is not prevented).  A ten-deep ista stack should allow sparsity to be achieved without the nonnegativity constraint.

WHY DOES A DEEP ISTA STACK LEAD TO SUCH DIFFERENT RESULTS?
NORMALIZATION OF MATRIX COLUMNS WAS INCORRECT: A minimum rather than a maximum norm was applied.  With the correct maximum norm, initial errors and gradients become huge, and training is horrible.
When using min rather than max (incorrectly), there isn't a problem, even when all columns are initialized to a norm of 1, and despite the fact that norms change very slowly.
decoding_pooling_dictionary seems to be initialized incorrectly


To satisfy Yann's desire for simplicity, try presenting closed-form loss function with position units factored out.
Check that reconstructions from pooling units (and higher layers) are sensible.

Is explicit normalization of layer pooling outputs required now that the columns of the decoding dictionaries are properly normalized?  Probably, especially because of the classification loss, which will otherwise directly operate on the magnitude of the encoding dictionaries and explaining_away.  Explaining_away in particular would likely be unstable, since it can induce output magnitude to increase exponentially if any of its eigenvalues are greater than one.
Row norms of the feature encoding dictionary grow larger than one.  In this case, the explaining-away matrix should potentially have a diagonal less than zero.  Bounding the magnitude of the feature encoding dictionary rows at 1 keeps the explaining away diagonal from going (strongly) negative, even if this constraint is not explicitly enforced.  Increasing row norms in the encoding_feature_extraction_dictionary is associated with decreasing column norms in the decoding_feature_extraction_dictionary.  Fixing the magnitude of the decoding_feature_extraction_dictionary columns to one, rather than merely bounding them above by one, does not keep the row norms of the encoding_feature_extraction_dictionary from growing larger than one.  Most of the gradient seems to be driven by the classification loss.  When the classification loss is removed, the rows of the encoding_feature_extraction dictionary do not grow large, and explaining_away retains its diagonal structure.  However, most of the feature extraction units are not trained.  It seems as if sparsity and pooling reconstruction loss are minimized by only using a small subset of the feature extraction units, so the pooling reconstruction can easily reconstruct all of them.

Link encoding and decoding dictionaries?  Would still need to account for scaling factor difference.

Confirmed that using only reconstruction and sparsity on the feature extraction layer, just about every feature_extraction unit is trained.  This is preserved when pooling reconstruction is added, then pooling sparsity, then mask sparsity (all with position loss set very small).  With mask sparsity and minimal position loss, most units train, but many learn bad, high-frequency features.  Turning up the position loss from 0.001 to 0.01 substantially reduces the number of feature extraction units trained.  This issue may be that pooling reconstruction effectively puts an L2 loss on the feature extraction outputs, divisively scaled by the pooling reconstruction.  If the pooling reconstruction of a unit is small, then this will tend to silence the feature extraction unit.  Consider the possibility that few features train since the units are forced to zero by the pooling and mask sparsity, combined with the requirement that all feature extraction units be equal to zero if their pooling reconstruction is equal to zero.  We turned the pooling and mask sparsity up so high because it was necessary to get sparsity amongst the features from the very beginning, but we pay a steep price for this.  What if we allow sparsity to develop more gradually?  Will it do so?

Even when we turn down pooling sparsity (and initialize the pooling dictionary to consist of groups of consecutive elements), some feature extraction elements don't train, others become high frequency, and most disturbingly, the pooling dictionaries lose all coherence.  On the plus side, the diagonal of explaining_away doesn't go negative, even though it is not constrained.  If we disable training of the pooling dictionaries, only a few rows of feature extraction elements train, and the feature extraction layer is extremely sparse, suggesting that normalization is too strong.  Because the reconstruction pooling error is L2, either every element must be reconstructed initially, the alpha control on the maximum reconstruction pooling error needs to be relaxed, or some mechanism needs to be introduced to allow sparsification of the pooling units directly.  Otherwise, those pooling units that are randomly less active on average will suppress their feature extraction inputs, which will learn to turn off, and these will never be trained.  

Mechanism of failure is as follows: the pooling sparsity causes the network to learn to activate some pooling groups more than others.  The less active pooling groups don't provide pooling reconstruction support, so the corresponding feature-extraction units are induced to be small with an L2 norm rather than a more forgiving L1 norm.  This strongly decreases their activity, inducing a positive feedback cycle that is only broken by the L2 feature extraction reconstruction loss.  The pooling sparsity terms alone are fine, since they are heavy-tailed and just correspond to group sparsity.  While we want to encourage the feature extraction units activated by an input to be compatible with the pooling reconstruction, we do not want to silence feature extraction units entirely just because the pooling reconstruction is bad.  

If we use the full reconstruction rather than the shrink reconstruction, there is no benefit to silencing useful feature extraction units (except that they won't be available for the pooling reconstruction, so units available for both the feature extraction and the pooling reconstruction will be favored).  Feature extraction units without pooling support are increased if they improve the feature extraction reconstruction without hurting the pooling reconstruction.  The full pooling reconstruction is like the feature extraction layer reconstruction, but the feature extraction units are scaled down if their pooling reconstruction is much smaller than alpha.  If alpha is very small, it will be possible to just enable all units with the pooling layer; with more reasonable alpha, there should be the classic balance between L1 and L2 norms.  Importantly, a feature extraction unit is never pushed down simply because its pooling support is small; rather, it is pushed up if the pooling support is too small to allow correct reconstruction.  

Even with full pooling reconstruction and full position loss, rather than shrink-based losses, only a few feature extraction units are trained sensibly; the rest are high-frequency, with weak encodings.  Strangely, the pooling dictionary changes little, even when it is trainable.  The feature extraction units that are trained are located in two or three contiguous blocks, suggesting that only a few pooling units grow strong enough to support reconstructions (the alternate explanation, that this increases the pooling sparsity, is inconsistent with good training when pooling reconstruction losses are disabled, as explained below).  Even if feature extraction units are not directly disabled by insufficient pooling reconstruction, large decoded inputs that well-reconstructed induce large L2 position loss.  

When pooling reconstruction and position losses are disabled, all units train.  The best-trained units are not obviously in contiguous blocks, and adjacent units appear only vaguely similar, but the pooling output is in fact sparse, with a few contiguous elements that are about 10 times larger than the rest.  Consider the possibility that it's just too difficult to make the pooling units large enough to reconstruct the inputs (or feature extraction units) with sufficient magnitude.  In particular, pushing the pooling units larger would make the pooling sparsity loss larger than the pooling reconstruction loss.  Without pooling reconstruction loss, the pooling reconstruction rarely exceeds 0.2, and (0.2)^2 << 0.1.  

Turning down the L2 position loss from 0.1 to 0.01 substantially increases the number of feature extraction units that train, but a substantial fraction are still silenced.  The pooling output is very sparse, with many outputs almost equal to zero.  This seems related to the observation that the L2 position loss has a maximum at sqrt(lambda_position / lambda_pooling_reconstruction); some feature extraction units minimize the L2 position loss by making the pooling reconstruction large, others by making the pooling reconstruction small.  We really want the network to learn to vary which are large and which are small based on the input.  The magnitude of the rows of the encoding_feature_extraction_dictionary seems to increase consistently with training, suggesting that the initialization to 1/num_ista_iterations is too small.

Decreasing both the scaling factor on the pooling reconstruction and pooling position loss, which maintains alpha but scales down the magnitude of the pooling reconstruction losses, allows all feature extraction units to train, but makes the pooling reconstruction gradients so small as to be irrelevant.  Keep in mind that if a row of the encoding_feature_extraction_dictionary is large, but it is not reconstructed by pooling, then it cannot contribute to the pooling reconstruction, and the to whatever degree that feature extraction unit reduces the reconstruction error of the feature extraction loss, it will not do so for the pooling reconstruction loss.  If we remove the FE reconstruction loss, then we do not need to split this mismatch, and feature extraction units can grow large only bounded by the L1 loss.  Turning off the feature extraction reconstruction loss without changing the L1 losses causes activity to crash to zero, indicating that it was the strongest force in the network, and the pooling reconstruction was weak in comparison.  When the L1 losses are scaled down to match, some pooling units are always turned off, and the corresponding feature extraction units do not train.  

After turning off all pooling sparsity, some pooling units are still consistently made very small, and the associated feature extraction units do not train strongly; this tendency seems to increase with training.  The remaining pooling outputs become uniform and large, thereby supporting all possible reconstructions.  If we turn up the initial scaling on the encoding_pooling_dictionary, only the very last feature extraction and pooling units, which do not have the same range of inputs as the others, fail to be active.

Keep in mind that the reconstruction pooling is necessary primarily to counter the effects of group sparsity on trainable pooling.  At the current balance of loss functions, the decoding_pooling_dictionary becomes non-sparse, indicating that the pooling reconstruction loss is too strong relative to the L1 losses, can can be reduced, thereby also allowing all feature extraction units to train.  Note though that while reducing the magnitude fo the pooling reconstruction loss may be an effective strategy with one layer, once we start stacking layers, the strong feature extraction reconstruction loss will dominate the pooling dictionaries, and force them to produce outputs that are easy to reconstruct.  

We want the pooling reconstruction to be big wherever the input is big, using as few pooling dictionary elements as possible, with each pooling element being as sparse as possible, and when there is a conflict between the pooling reconstruction and the input, we *always* want to change the pooling reconstruction.  This can be achieved by making the pooling reconstruction loss much smaller than the feature extraction reconstruction loss.  In practice, it doesn't seem to be necessary to make the scaling on the pooling reconstruction loss much less than 1, so long as the L2 position loss is small, making the threshold that the pooling reconstruction must pass to drive down the pooling reconstruction and L2 position losses relatively low.  

The main difference between the 9/28 runs and the 9/24 runs is that the L2 position norm has been reduced from 0.1 to 0.01, implying that the pooling reconstruction needs to be about 0.1, rather than about 0.3; and on top of that both the pooling reconstruction loss and pooling L2 position loss have been reduced by a factor of between 0.5 and 0.125, to ensure that the shrink reconstruction is more important than the pooling reconstruction.  I need to identify the point at which the reduced pooling reconstruction loss is no longer sufficient to balance the pooling L1 norms, and the magnitudes of the encoding_pooling_dictionary rows begin to fall significantly over the course of training.  Scaling down both pooling reconstruction loss and L2 position loss by the same amount does not have any obvious effect on filter structure (9/28 runs).  However, pooling row norms fall considerably; to as low as 0.5 (with 0.125 scaling) or 0.7 (with 0.5 scaling) from 2.  When using accelerated training of encoding_ and decoding_pooling_dictionary (10x and 50x, respectively), some pooling row norms collapse almost to zero, and perhaps half are below 0.5.  Correspondingly, the rows of the encoding_pooling_dictionary generally only have a single non-zero elements, and the decoding_pooling_dictionary tends to have one large (matching) element, and a scattering of smaller non-zero elements.  Over half of the feature extraction units train poorly, without a single smooth, spatially localized, well-defined shape.  

Alternatively, consider the possibility that mismatches between the sparse encoding and the pooling can be rectified primarily using the pooling dictionary by increasing the learning rate of the pooling dictionaries.  This presumably corresponds to optimizing the feature extraction dictionaries at something like a local minimum of the pooling dictionaries.  When tried on 9/28, this doesn't work well; many feature extraction elements are not trained to correspond to well-formed features.

Since pure group sparsity seems to work well, check whether the problem is the pooling reconstruction loss or the training of the pooling weights.  Run three simulations, one of which has all L1 norms but fixed pooling dictionaries, the next of which adds pooling reconstruction but leaves pooling dictionaries fixed, and the last of which has both pooling reconstruction loss and trainable pooling dictionaries.  These tests differ from similar tests on 9/27, in that the L2 position loss and thus alpha is much smaller, which should keep pooling units and their corresponding feature extraction units from being driven to zero, as described below.

Large alpha (ratio of L2 position to pooling reconstruction loss) is problematic, since it makes initial increases in the pooling outputs relatively unprofitable, assuming they start too small; any increase in the pooling reconstruction is small compared to alpha, and so has a minimal effect on the pooling reconstruction loss.  In contrast, the gradient of the L1 loss on the pooling units has a constant gradient regardless of the size of the pooling reconstruction, and so the pooling units are driven towards zero.  At the same time, if the scaling factor on the pooling reconstruction loss is large, a small pooling reconstruction implies that large feature extraction units are heavily penalized.  The combination of a large alpha (compared to the initial pooling reconstructions) and a large scaling factor on the pooling reconstruction loss act to drive most of the feature extraction units to zero.  

A large part of the problem of the pooling dictionary seems to be that the encoding rows can vary in magnitude; when magnitude decays, the importance of both that pooling unit and its reconstruction decreases.  What if we normalized the magnitude, either of the encoders, or of the pooled output before?  Normalizing the magnitude of the encoding_pooling_dictionary pulls us a step closer to the pure group sparsity case, which actually works.  It makes less sense to normalize the magnitude of the rows of the encoding_feature_extraction_dictionary, since the overall magnitude of feature extraction can also be modulated by the explaining_away tensor and the parameterized_shrinks.

It seems like the right level of L1 loss corresponds to the point where the encoding_feature_extraction_dictionary rows begin decreasing in magnitude, rather than remaining pegged at the maximum 1.25/num_ista_iterations to which they are initialized.

The relative magnitudes of the loss gradients at their source does not seem to be a good estimate of their effect.  When we add the pooling reconstruction loss into group sparsity, the pooling reconstruction induces a much smaller direct gradient than the group sparsity, but the pooling reconstruction loss falls consistently, whereas the group sparsity actually tends to increase initially, although it begins to fall consistently after a couple tens of epochs of 5000 datapoints.

When the pooling dictionaries are made trainable, the pooling L1 losses decrease (even with less training) while the pooling reconstruction loss increases many times over, relative to the those obtained with the same pooling reconstruction loss but fixed pooling dictionaries.  Presumably, the encoding pooling dictionary is altered to minimize the magnitude of the pooling units.  Empirically, the encoding_pooling_dictionary becomes sparse, and the decreased elements seem to correspond to strongly trained feature extraction units.  Of course, this will also reduce the quality of the pooling reconstruction, since it will not increase correspondingly to account for these large feature extraction units.  We can presumably reduce this spurious reduction of the pooling L1 losses by increasing the scaling on the pooling reconstruction loss, thereby requiring that the encoding_ and decoding_pooling_dictionary accurately reflect the feature extraction unit statistics.  Check if this reduces the magnitude of the gradient on the encoding pooling dictionary, which substantially increased when we turned on trainable pooling.

Alternatively, we could counter the decrease in the L1 loss functions with the advent of trainable pooling by increasing the scaling factor on the L1 loss functions directly.  That is, we accept that there will be some spurious decrease in the L1 loss functions because of cheating due to reconfiguration of the pooling dictionary.  This cannot be wholly avoided with any sensible amount of pooling reconstruction loss, so just reverse the decrease directly.  On the other hand, making pooling trainable makes it easier to satisfy both the L1 sparsity and pooling reconstruction constraints.  As a result, more effort will be devoted to these loss functions, since they can be more effectively reduced.  This is consistent with the large gradient on the encoding_pooling_dictionary.  The appropriate counter is to increase the feature extraction reconstruction loss, since otherwise it will be neglected, since gains are harder to achieve.  Correspondingly, the feature extraction reconstruction loss increases when trainable pooling is enabled.  We can only force it back down (and thus force the other losses back up, since there is a trade-off between them) by increasing the feature extraction reconstruction loss.  The same argument should then apply to the pooling reconstruction, since empricially, it also increases when the pooling dictionary is made trainable, and the percentage change is much larger than for the feature extraction reconstruction; it's easier to decrease the pooling L1 loss than to decrease the pooling reconstruction loss by changing the pooling dictionary.

Untrained pooling: 
pooling_reconstruction_scaling = 0.5
rec_mag = 4
L1_scaling = 7.5 

Trained pooling:
pooling_reconstruction_scaling = 1
rec_mag = 6
L1_scaling = 7.5 


A small alpha (ratio of L2 position loss to pooling reconstruction loss) may be even more important when the pooling dictionaries are trainable, since if the decoding_pooling_dictionary projection to a feature extraction unit is reduced since it is not yet significantly used, it will be hard to recover if the threshold set by alpha is not very small.
It seems like the real difficulty is posed by making the encoding_pooling_dictionary trainable.  This quickly learns to be sparse, eliminating connections from some feature extraction units entirely, presumably reducing the L1 pooling norm.  Counterintively, the only resolution appears to be reducing the pooling L1 norms, which is equivalent to increasing the reconstruction norms (and scaling down the learning rate), and puts more pressure on the network not to cheat by using a pooling output that is insufficient to reconstruct the input.  It seems plausible that the relative importance of feature extraction reconstruction should be reduced along with that of pooling sparsity, since it is the pooling reconstruction alone that is being contravened by a poor encoding_pooling_dictionary.  Consider using the parameters that worked well with fixed pooling, and just increasing the scaling of pooling reconstruction, possibly while simultaneously reducing alpha.  A simple increase of pooling reconstruction loss should be sufficient so long as only the encoding_pooling_dictionary is trained; when the decoding_pooling_dictionary is also trained, alpha reduction will be more important to avoid vicious positive feedback cycles that disable relatively bad feature extraction units.  



Be careful about the L2 position loss; if alpha is much larger than the squared reconstruction, it increases linearly with the reconstruction.  

With the switch to full pooling reconstruction from shrink pooling reconstruction, the encoding_feature_extraction_dictionary has come to closely resember the decoding_feature_extraction_dictionary, rather than having weird diffuse elements with strongly negative regions.  

What happens if we initialize the decoding pooling dictionary to be uniform?




If both position and reconstruction loss are very small, the encoding pooling dictionary goes to zero as expected.
What if we normalize the feature extraction outputs before pooling?
What if we use something like cosine between (normalized) feature extraction input and pooling output?
Pure group sparsity does not appear to be as good as on the last attempt.  What parameters changed?




Need to use 0.5e-3 rather than 5e-3 when training with classification loss alone

Jia, Huang, Darrell (CVPR 2012) Beyond Spatial Pyramids: Receptive Field Learning for Pooled Image Features




TRY ONLY NORMALIZING BEFORE THE CLASSIFICATION LOSS
TRY ONLY ADJUSTING THE SECOND LAYER L1 LOSS WHEN ADDING A SECOND LAYER
TRY ADJUSTING INIT POOLING SPARSITY RATHER THAN THE L1 LOSS SCALING WHEN INCREASING THE NUMBER OF UNITS (the problem with this is that it seems to generate feature extraction units that aren't connected to any pooling units, especially if the number of pooling units isn't also increased.  

CONSIDER using overcomplete pooling, as suggested by Jia, Huang, Darrell (2012); scale down L1 loss appropriately
Identify the slowest component of the network.  If it is still repair of the matrices after training, write C code to do this
How many times should repair be called?
Are square and sqrt actually slow on the server?

Large learning rates after classification loss is enabled seem to initially bounce the network out of a local minimum, leading to large learning rates.  This occurs even if we simply fail to account for the effect of learning rate decay, which decreases the learning rate by a factor of 6 with the current default parameters.  The network resettles into a new local optimum fairly quickly.  Interestingly, the largest gradients are seen in the encoding feature extraction dictionary and the parameterized shrink; explaining away also develops large gradients.  The new local optimum looks very similar to the original one.  The biggest differences seem to be in the encoding feature extraction dictionary elements that are heavily changed by the classification loss.

The good run on 10/11 had normalized rows in encoding_feature_extraction_dictionary.  However, this is a heuristic and unmotivated mechanism to keep the encoding_feature_extraction_dictionary from exploding.  It would be cleaner to decrease the strength of the classification loss relative to the reconstruction losses, thereby forcing the network to solve classification given the constraint that reconstruction remains good.
Perhaps encoding feature extraction dictionary elements distort because otherwise the output is insufficiently nonlinear.  Note that the pooling output is generally not very sparse, and the classification dictionary tends to be dense, indicating that most pooling units are associated with every digit.
The previous test in which I conlcuded that a classification loss scaling of 0.1 was insufficient seems to have only been run on the quick training set (500 elements)!

10/11 random_init run - 220 epochs, 97.77% - 280 epochs, 98.38% - 320 epochs, 98.28% - 380 epochs, 98.37%

10/22 1en5_learning_rate run - 280 epochs, 97.90% (99.66% on training set) - 340 epochs, 97.88% (99.88% on training set) - 380 epochs, 97.93% - 400 epochs, 96.17% (97.92% on training set) - 420 epochs, 97.97% - 440 epochs, 97.97% (99.96% on training set)
10/23 2en5_leaning_rate run - 260 epochs, 97.34% - 280 epochs, 97.95% - 300 epochs, 97.77% correct (99.90% on training set) - 420 epochs, 97.97% - 440 epochs, 97.97% (99.96% on training set)
10/23 2en5_learning rate, fixed_shrink, 1 classification scaling run - 440 epochs, 98% - 480 epochs, 97.93% - 520 epochs, 97.97% - 680 epochs, 97.95%

10/25 2en5_learning_rate, 200 pooling units, fixed_shrink, 1 classification scaling run - 340 epochs, 97.98% - 360 epochs, 97.76% - 540 epochs, 97.82%
10/25 2en5_learning_rate normalized encoding_feature_extraction_dictionary rows, ista sparsity 1, fixed_shrink - 260 epochs, 96.57% - 280 epochs, 98.08% - 300 epochs, 97.62% - 320 epochs, 98.17% - 340 epochs, 98.32% - 360 epochs, 98.35% (99.9% on training set) - 380 epochs, 98.34% - 400 epochs, 98.52% - 460 epochs, 98.52% - 480 epochs, 98.43% - 500 epochs, 98.53% - 600 epochs, 98.5% - 640 epochs, 98.49%

10/26 2en5_learning_rate, 0.2 classification scaling, fixed_shrink run - 340 epochs, 97.76% (99.8% on training set) - 360 epochs, 97.72% - 380 epochs, 97.72% (99.82% on training set) - 500 epochs, 97.66% - 540 epochs, 97.62% - 680 epochs, 97.62%

10/30 2en5_learning_rate, short classifier pretraining, restart from 10/23 fixed shrink 1 classification - 60 epochs, 97.88% - 80 epochs, 97.89% - 100 epochs, 97.89% - 120 epochs, 97.93% - 140 epochs, 97.9% - 160 epochs, 97.96% - 240 epochs, 97.9%
10/30 2en5_learning_rate, long classifier pretraining, restart from 10/23 fixed shrink 1 classification - 40 epochs, 97.58% - 60 epochs, 97.84% - 80 epochs, 97.77% - 100 epochs, 97.9% - 120 epochs, 97.98% - 140 epochs, 97.93% - 240 epochs, 97.87% - 280 epochs, 97.9%

10/30 (actually 10/31) normalized encoding feature extraction run restarted (from 10/25) WITH classifier pretraining - 20 epochs, 97.37% - 40 epochs, 97.71% - 80 epochs, 97.8% - 100 epochs, 97.99% - 120 epochs, 98.06% - 140 epochs, 98.11% - 160 epochs, 97.99% - 180 epochs, 98% - 240 epochs, 98.1% - 260 epochs, 97.86% - 280 epochs, 98.04%
10/31 - both encoding feature extraction rows and encoding pooling rows normalized, trained from scratch WITH classification pretraining - 240 epochs, 97.85% - 260 epochs, 97.53% - 280 epochs, 98.26% - 460 epochs, 98.28% - 480 epochs, 98.15% - 500 epochs, 98.24% - 620 epochs, 98.27% - 640 epochs, 98.19%
10/31 - learning rate of ista modules scaled down in proportion to the number of repeats, trained from scratch WITH classification pretraining - 220 epochs, 96.55% - 240 epochs, 97.94% - 260 epochs, 97.76% - 660 epochs, 97.81% - 680 epochs, 97.76%

11/1 short classifier pretraining, restart from 10.25 normalized encoding FE run, continue to normalize encoding FE rows, 0.1 classification after pretraining - 60 epochs, 96.83% - 80 epochs, 97.12% - 100 epochs, 97.11%
11/1 short classifier pretraining, restart from 10.25 normalized encoding FE run, do not normalize encoding FE rows, 0.1 classification after pretraining - 80 epochs, 97.04% - 100 epochs, 97.1% - 120 epochs, 97.11%
11/1 only two ista iterations WITH classification pretraining (from scratch) - 300 epochs, 97% - 400 epochs, 97.19% - 440 epochs, 97.11% - 520 epochs, 97.24%
11/1 encoding FE rows fully normalized, no classification pretraining (from scratch) - 321 epochs, 98.18% - 341 epochs, 97.44% - 361 epochs, 98.01% - 460 epochs, 98.44% - 480 epochs, 98.35% - 500 epochs, 98.51% - 520 epochs, 97.83% (!!!) - 540 epochs, 98.45% - at 520 epochs, many 5's are mistaken for 0's


RESTART RUNS ALMOST CERTAINLY HAVE A LEARNING RATE THAT IS TOO LARGE!!!  This may explain their lack of success
Even with the learning rate decay set properly, the validation loss is unstable.  This strongly suggests that the learning rate is too large, and should be reduced.  This is despite the fact that receptive fields do not visually appear to change significantly over the course of 20 epochs of training, and the fact that it takes on the order of 100 epochs of supervised training before the validation error plateaus.  

11/2 restart from 10.25, normalized encoding FE - 100 epochs, 97.97% - 200 epochs, 98.39% - 220 epochs, 98.33% - 240 epochs, 98% - 280 epochs, 98.28% - 300 epochs, 98.42%
11/2 duplicate 10.25 from scratch, normalized encoding FE - 240 epochs, 96.54% - 260 epochs, 97.27% - 280 epochs, 98.35% - 340 epochs, 98.01% - 360 epochs, 98.51% - 380 epochs, 98.31%
11/2 restart from 10.31, both encoding FE and encoding P normalized - 140 epochs, 98.45% - 160 epochs, 97.88% - 180 epochs, 98.36% - 200 epochs, 98.47% - 220 epochs, 97.99% - 260 epochs, 97.47% - 280 epochs, 98.39% - 300 epochs, 98.38% - 320 epochs, 98.15% - 440 epochs, 98.08% (!!!) - 460 epochs, 98.43% - 540 epochs, 98.32% - 560 epochs, 98.45% (98.72% on test set) - 580 epochs, 98.39% (98.65% on test set)
11/2 300 FE units, normalized encoding FE - 240 epochs, 97.87% - 260 epochs, 97.37% - 280 epochs, 97.44% - 300 epochs, 98.38% - 320 epochs, 98.33% - 340 epochs, 98.38% - 360 epochs, 98.41% - 380 epochs, 98.45% - 400 epochs, 98.53% - 420 epochs, 98.5% - 440 epochs, 98.52% (98.54% on test set) - 500 epochs, 98.45% - 520 epochs, 98.5% - 580 epochs, 98.26% - 600 epochs, 98.49% - 620 epochs, 98.44%

11/5 restart (from end of 11/2 fresh normalized encoding FE run) with 5e-4 learning rate - 0 epochs, 98.44% - 80 epochs, 98.4% - 100 epochs, 98.4% - 120 epochs, 98.39% - 140 epochs, 98.39% - 160 epochs, 98.4% - 180 epochs, 98.39% - changes to the encoding_feature_extraction_dictionary are minimal, especially after the first 20 epochs
11/5 restart 10/25 (after unsupervised pretraining) with 1e-3 learning rate - killed since machine was hosed
11/5 0.375e-2 mask_mag, 0.1e-2 pooling_sl_mag, 0 sl_mag, normalized encoding FE from scratch - looked good before the server died.  If the 0.35e-2 mask mag run doesn't produce encoder reconstructions that all look even and full, try redoing this run
11/5 restart 10/25 (after unsupervised pretraining) with 0.5 classification loss (normalized encoding FE) - 80 epochs, 97.82% - 100 epochs, 97.88% - 120 epochs, 98.11% - 140 epochs, 98.1% - 160 epochs, 97.82% - 180 epochs, 93.5% (!!!?!) - 200 epochs, 97.98% - 220 epochs, 98.13% - 240 epochs, 98.21% - 260 epochs, 97.47% - there are still noticable changes in the encoding_feature_extraction_dictionary between saves - at 180 epochs, there is massive bias towards returning a classification of 4, suggesting that the network made much too large of a correction in this direction!!!
11/5 only two ista iterations, normalized encoding FE from scratch - 260 epochs, 97.47% - 300 epochs, 97.84% - 320 epochs, 98% - 340 epochs, 98.09% - 360 epochs, 98.05% - 460 epochs, 98.07% - 480 epochs, 98.04% - 500 epochs, 98.14%
11/5 no pooling, enc rows normalized, 9e-2 sparsity - 260 epochs, 95.02% - 280 epochs, 98.45% (98.26% on test set) - 300 epochs, 98.21% (98.11% on test set) - 320 epochs, 98.55% (98.47% on test set) - 340 epochs, 98.18% - 360 epochs, 98.55% - 380 epochs, 98.5% - 400 epochs, 98.65% (98.74% on test set) - 420 epochs, 98.16% - 440 epochs, 98.48% - 460 epochs, 97.94% - 520 epochs, 98.39% (probably test by accident) - 540 epochs, 98.71 (98.8% on test set) - 560 epochs, 98.72% (98.75% on test set) - 580 epochs, 97.35% - 1020 - 98.62% - 1040 epochs, 97.81% - 1060 epochs, 98.53% - 1140 epochs, 98.51% - 1160 epochs, 98.54% - 1180 epochs, 98.34%
But keep in mind that we probably shouldn't expect a huge benefit to pooling with only a single sparse coding layer, since the classification dictionary already effectively performs L1 pooling, since the sparse code is non-negative.  The real advantage would come from multiple alternating layers of sparse coding and L2 pooling.  
Classification dictionary scaling may be increasing faster than the learning rate is decaying, leading to instability as training progresses
Dictionary elements that are poorly trained during unsupervised pretraining seem to be kicked out of their local optimum during supervised training, and almost all develop meaningful receptive fields.

11/6 restart 10/25 from end of unsupervised pretraining with 0.5 classification loss (normalized encoding FE) - 60 epochs, 97.45% - 80 epochs, 97.83% - 100 epochs, 97.99% - 120 epochs, 98.17% - 140 epochs, 98.13% - 160 epochs, 97.91% - 180 epochs, 91.33% (!!! - massive bias towards 4) - 200 epochs, 98.07% - 280 epochs, 98.23% - 300 epochs, 98.16% - 320 epochs, 98.18% - changes over the course of learning are small after the first 40 or so epochs.  The poor performance of this network (even compared to that with a globally halved learning rate) suggests that, if anything, the balance between regularization and classification is skewed too far towards regularization
11/6 restart 10/25 from end of unsupervised pretraining with 1e-3 learning rate (normalized encoding FE) - 100 epochs, 98.04% - 120 epochs, 98.12% - 140 epochs, 98.09% - 160 epochs, 96.41% (!!! - strong bias towards 9 and 0) - 180 epochs, 89.59% - 200 epochs, 98.2% - 220 epochs, 98.28% - 280 epochs, 98.39% - 300 epochs, 98.3% - 320 epochs, 98.31% - 340 epochs, 98.31% - 360 epochs, 98.35% - 380 epochs, 98.31% - 460 epochs, 98.3% - 480 epochs, 97.49% (!!!) - 500 epochs, 98.38% - 520 epochs, 98.32%
11/6 learning rate 4e-2 during pretraining, 2e-2 during supervised classification training 
11/6 0.35e-2 mask_mag, 0.1e-2 pooling_sl_mag, 1 sl_mag, normalized encoding FE - 280 epochs, 98.03% - 300 epochs, 98.18% - 320 epochs, 98.11% - 340 epochs, 97.58% - 360 epochs, 98.36% - 380 epochs, 97.99% - 460 epochs, 98.51% - 480 epochs, 96.94% - 500 epochs, 98.38% - 520 epochs, 98.4% - I suspect this works (slightly) less well than with smaller mask sparsity since it forces the decoding pooling dictionary to be sparse, so the encoding pooling dictionary must be relatively non-sparse to produce good pooling reconstructions, and the pooling units cannot focus on the minority of whole-digit-classifying sparse coding units that best predict the classification.  On the other hand, the data set seems to be more evenly distributed across the pooling units, and the encoding pooling dictionary rows actually pool over multiple units, as desired in principle.

11/7 300 FE units, 75 P units, normalized encoding FE - killed prematurely
11/7 no pooling, enc rows normalized, 9e-2 sparsity, 300 fe units - killed prematurely
11/7 learning rate 4e-3 - half of the encoding_feature_extraction_dictionary rows are not trained!  It appears as if 4e-3 is too large
11/7 two recpool layers, 2.25 L1 scaling, 0.2 scaling of second layer

11/8 restart 11/5 no pooling, enc rows normalized, 9e-2 sparsity, but with classification loss scaling 2 and learning rate 1e-3 - 100 epochs, 98.41 (98.42% on test set)- 120 epochs, 98.42% (98.55% on test set) - 140 epochs, 98.11% (98.42% on test set) - 160 epochs, 98.62% (98.71% on test set) - 180 epochs, 98.11% (98.11% on test set) - 200 epochs, 98.72% (98.7% on test set) - 680 epochs, 98.56% - 700 epochs, 98.49% - 720 epochs, 98.57% - 740 epochs, 98.47% - 760 epochs, 98.45% - 780 epochs, 98.5% - 800 epochs, 98.49% - 960 epochs, 98.5% - 980 epochs, 98.47% - 1000 epochs, 98.47%
11/8 no pooling, enc rows normalized, 9e-2 sparsity, only 100 epochs of unsupervised pretraining - 440 epochs, 98.52% - 460 epochs, 98.68% - 480 epochs, 98.76% (98.7% on test set) - 500 epochs, 98.57% - 520 epochs, 98.46% - 660 epochs, 98.65% - 680 epochs, 98.67% - 700 epochs, 98.67%
11/8 no pooling, enc rows normalized, 9e-2 sparsity, 300 fe units - 340 epochs, 98.94% (98.78% on test set) - 360 epochs, 97.53% - 380 epochs, 97.86% (98.944% on train set) - 400 epochs, 98.82% (98.63% on test set, 99.994% (all but 3) on train set) - 440 epochs, 98.81% (98.73% on test) - 460 epochs, 98.72% - 480 epochs, 98.81% (98.64% on test) - 500 epochs, 98.76% - 520 epochs, 98.78% (98.65% on test) - 540 epochs, 98.8% (98.67% on test) - 600 epochs, 98.76% (98.72% on test) - 620 epochs, 98.75% (98.7% on test) - 680 epochs, 98.74% (98.6% on test set) - 700 epochs, 98.78% (98.58% on test set)
11/8 no pooling, enc rows normalized, 9e-2 sparsity, only 2 epochs of unsupervised pretraining, learning rate 1e-3 - 140 epochs, 98.36% - 540 epochs, 98.38% - 560 epochs, 98.4% - 580 epochs, 98.31% - 600 epochs, 98.34% - 780 epochs, 98.28% - 800 epochs, 98.3% - 820 epochs, 98.25%

11/9 no pooling, enc rows normalized 1.5, 9e-2 sparsity - 480 epochs, 98.55% - 500 epochs, 98.67% - 520 epochs, 98.61% - 540 epochs, 98.5% - 560 epochs, 98.54% - 580 epochs, 98.48% - 800 epochs, 98.51% - 820 epochs, 98.55% - 840 epochs, 98.46%
11/9 no pooling, enc rows normalized 1.25, 9e-2 sparsity (should replicate 11/5) - 460 epochs, 98.3% - 480 epochs, 98.83% (98.68% on test set) - 500 epochs, 98.74% - 520 epochs, 98.39% - 540 epochs, 98.59% - 560 epochs, 98.61% (99.982% (all but 9) on train set) - 580 epochs, 98.3% (99.686% on train set) - 820 epochs, 98.6% - 840 epochs, 98.65% - 860 epochs, 98.05%
11/9 no pooling, enc rows normalized 1.0, 9e-2 sparsity - 380 epochs, 98.44% - 400 epochs, 98.67% - 420 epochs, 98.25% - 540 epochs, 98.58% - 560 epochs, 98.54% - 580 epochs, 98.09% - 600 epochs, 98.64%
11/9 no pooling, enc rows normalize 1.25, 9e-2 sparsity, 11 ista iterations - 340 epochs, 97.18% - 360 epochs, 97.72% - 380 epochs, 98.52% - 520 epochs, 98.55% - 540 epochs, 96.66% - 560 epochs, 98.53% - 640 epochs, 95.01% - 660 epochs, 96.51% - 680 epochs, 98.72%

Pretraining does seem to be important, but 100 epochs of pretraining seems to be sufficient.  300 hidden units probably helps, but learning is no more stable, and will need to be investigated further.  There is no obvious effect to small variations in the amount of encoding_feature_extraction_dictionary row normalization.  Increasing the scaling of the classification loss does not hurt; decreasing the scaling of the classification loss probably does hurt.  

Complex cell units seem to be much less dependent than simple cell units on their direct sensory input.  Whereas simple cell units rarely develop significant positive outputs if their feedforward activation is not positive, complex cell units that grow large often have negative feedforward activation.  

11/13 minibatch size 10, learning rate 10e-2, 100 epochs unsupervised - 200 epochs, 97.22% - 220 epochs, 97.25% - 240 epochs, 97.57% - 500 epochs, 97.86% - 520 epochs, 97.92% - 540 epochs, 97.88% - 560 epochs, 97.68%
11/13 minibatch size 10, learning rate 20e-2, 100 epochs unsupervised - 200 epochs, 97.12% - 220 epochs, 97.76% - 240 epochs, 96.23% - 260 epochs, 97.57% - 460 epochs, 97.86% - 480 epochs, 95.74% - 500 epochs, 97.36% - 520 epoch, 98%
11/13 400 hidden units minibatch size 10 learning rate 20e-2, 100 epochs unsupervised - 180 epochs, 98.25% - 200 epochs, 98.26% - 240 epochs, 98.33% - 260 epochs, 98.22%
11/13 minibatch size 10, learning rate 20e-2, 6e-2 sparsity, 100 epochs unsupervised - 380 epochs, 97.14% - 400 epochs, 97.79% - 420 epochs, 97.78% - 440 epochs, 97.85%
11/13 minibatch size 10, learning rate 20e-2, 12e-2 sparsity, 100 epochs unsupervised - 220 epochs, 97.49% - 240 epochs, 97.36% - 260 epochs, 97.44%

Learning rate decay is based upon the number of calls to SGD, rather than the number of datapoints evaluated.  To maintain parity with previous runs, we need to scale the learning rate decay by the minibatch size.  In general, training time can be used as a proxy for decreasing the learning rate, because of learning rate decay.  The 11/5 run thus demonstrates that decreasing the learning rate is not a panacea for instability in performance.  On the other hand, the amount of time required after pretraining to obtain good classification performance may be a result of the need to wait for learning rate decay to gradually reduce the learning rate.  
***I don't think I ever tried classifier pretraining with learning rate decay set properly.  This might be worth reinvestigating.  
Even with batch training with uncorrected learning rate decay, the encoding and decoding filters do not appear to be unstable.  The complex cell units may change more than the simple cell units, but both seem consistent on the order of 20 epochs.  Can we accelerate training by using a larger learning rate combined with larger learning rate decay?

11/14 minibatch size 10, learning rate 10e-2, 200 epochs unsupervised - 300 epochs, 96.8% - 320 epochs, 97.13% - 340 epochs, 97.21% - 400 epochs, 97.57% - 420 epochs, 97.05% - 440 epochs, 97.45% - 500 epochs, 97.7% - 520 epochs, 97.8% - 540 epochs, 97.88% - 600 epochs, 97.86% - 620 epochs, 97.9% - 700 epochs, 97.97% - 720 epochs, 98% -740 epochs, 97.9% - 760 epochs, 97.97% - 780 epochs, 98.06%
11/14 minibatch size 10, learning rate 10e-2, 200 epochs unsupervised, learning rate decay adjusted for minibatches - 300 epochs, 90.18% - 400 epochs, 94.04% - 500 epochs, 95.61% - 600 epochs, 96.28%
11/14 minibatch size 10, learning rate 20e-2, 100 epochs unsupervised, learning rate decay adjusted for minibatches - 300 epochs, 97.36% - 400 epochs, 97.51% - 500 epochs, 97.65% - 640 epochs, 97.74%
11/14 minibatch size 10, learning rate 20e-2, 200 epochs unsupervised, learning rate decay adjusted for minibatches - 300 epochs, 95.11% - 320 epoch, 95.78% - 340 epochs, 96.16% - 400 epochs, 96.83% - 500 epochs, 97.15% - 600 epochs, 97.4% - 680 epochs, 97.52% (monotonic increase since 300 epochs) - 760 epochs, 97.57%
11/14 minibatch size 10, learning rate 20e-2, 200 epochs unsupervised, doubled learning rate decay adjusted for minibatches - 640 epochs, 96.56% - 660 epochs, 96.59%

UP UNTIL NOW, the classification gradient (alone) has been scaled down in proportion to the size of the minibatch, yielding attrocious results

11/15 minibatch size 0 (matrix-vector), learning rate 2e-2, restart from 11/9 200 epochs, lrd adjusted 
11/15 minibatch size 1 (matrix-matrix), learning rate 2e-2, restart from 11/9 200 epochs, lrd adjusted 

11/15 minibatch size 10, learning rate 2e-2, 100 epochs pretraining, repair every 1 minibatch, fixed averaging of classification loss - 200 epochs, 97.97% - 220 epochs, 98.41% - 240 epochs, 98.61% (98.72% on test set) - 260 epochs, 98.36% - 280 epochs, 98.44% - 300 epochs, 98.59% - 320 epochs, 98.48% - 400 epochs, 98.49% - 420 epochs, 98.03% - 440 epochs, 98.6% - 460 epochs, 98.03%
11/15 minibatch size 10, learning rate 2e-2, 100 epochs pretraining, repair every 5 minibatches, fixed averaging of classification loss - 200 epochs, 97.36% - 220 epochs, 98.37% - 240 epochs, 98.09% - 280 epochs, 98.41% - 300 epochs, 96.92% - 320 epochs, 98.53% - 340 epochs, 98.6% (98.56% on test set) - 360 epochs, 97.82% - 380 epochs, 98.31% - 400 epochs, 98.58% - 460 epochs, 98.38% - 480 epochs, 98.58% - 500 epochs, 97.25% - 520 epochs, 98.65% - 540 epochs, 98.66% (98.6% on test set) - 560 epochs, 98.55% - 680 epochs, 98.66% (98.48% on test set) - 700 epochs, 98.62% - 720 epochs, 98.71% (98.5% on test set) - 800 epochs, 98.65% - 820 epochs, 98.7% (98.5% on test set) - 840 epochs, 98.66% - 860 epochs, 98.55% - 880 epochs, 98.61% - 1000 epochs, 98.54% - 1020 epochs, 98.31% (averaging 600 - 980, 98.7%)
11/15 minibatch size 10, learning rate 1e-2, 100 epochs pretraining, repair every 5 minibatches, fixed averaging of classification loss - 200 epochs, 98.28% - 220 epochs, 98.21% - 240 epochs, 98.44% - 300 epochs, 98.52% - 320 epochs, 98.46% - 340 epochs, 98.54% - 500 epochs, 98.39% - 520 epochs, 98.5% - 540 epochs, 98.44% - 560 epochs, 98.43% - 780 epochs, 98.38% - 800 epochs, 98.4% - 820 epochs, 98.4%
 11/15 minibatch size 10, learning rate 1e-2, 200 epochs pretraining, repair every 5 minibatches, fixed averaging of classification loss - 300 epochs, 98.17% - 320 epochs, 98.39% - 340 epochs, 98.11% - 400 epochs, 98.41% - 500 epochs, 98.38% - 540 epochs, 98.51% - 560 epochs, 98.62% - 580 epochs, 98.57% - 800 epochs, 780 epochs, 98.53% - 98.48% - 820 epochs, 98.44%
11/15 minibatch size 10, learning rate 1e-2, 200 epochs pretraining, repair every 1 minibatch, fixed averaging of classification loss - 300 epochs, 98.12% - 320 epochs, 98.28% - 440 epochs, 98.46% - 460 epochs, 98.49% - 800 epochs, 98.48% - 1140 epochs, 98.51% - 1160 epochs, 98.56% - 1180 epochs, 98.54%

Learning rate of 1e-2 is too slow, even with batches!  Many receptive fields remain noisy even after 460 epochs of supervised training, and receptive fields change relatively little even over hundreds of epochs of training.  

11/16 minibatch size 10, learning rate 2e-2, 100 epochs pretraining, repair every 5 minibatches, 400 hidden units  - 380 epochs, 98.7% - 400 epochs, 98.58% - 420 epochs - 500 epochs, 98.63% - 600 epochs, 98.57% - 700 epochs, 98.58% - 800 epochs, 98.48% - 840 epochs, 98.46% - 98.68% - 860 epochs, 98.5% - 880 epochs, 98.5% - 1120 epochs, 98.45% - 1140 epochs, 98.43% - 1160 epochs, 98.43% -1180 epochs, 98.42%
11/16 minibatch size 10, learning rate 2e-2, 200 epochs pretraining, repair every 5 minibatches, 400 hidden units - 250 epochs, 98.38% - 300 epochs, 98.76% - 350 epochs, 98.75% - 400 epochs, 98.76% - 450 epochs, 98.72% - 500 epochs, 98.8% - 600 epochs, 98.79% - 700 epochs, 98.77% - 800 epochs, 98.77% - 1100 epochs, 98.77% - 1150 epochs, 98.76%
11/16 minibatch size 10, learning rate 2e-2, 100 epochs pretraining, repair every 5 minibatches, averaged SGD (ASGD) with t0 = 400 epochs - 700 epochs, 98.54% - 800 epochs, 98.51% - 880 epochs, 98.5% - 900 epochs, 98.54% - 1080 epochs, 98.57% (98.62% on test)
Learning rate may still be too high!

11/16 minibatch size 10, learning rate 2e-2, 100 epochs pretraining, repair every 5 minibatches, fe row norm bound at 2, but initialized to 1.25 - 600 epochs, 98.19% - 650 epochs, 98.36% - 700 epochs, 98.36% - 750 epochs, 98.29% - 1000 epochs, 98.06% - 1050 epochs, 98.22%
11/16 minibatch size 10, learning rate 2e-2, 100 epochs pretraining, repair every 5 minibatches, fe row norm bound at 4, but initialized to 1.25 - 600 epochs, 98.12% - 650 epochs, 98.16% - 700 epochs, 98.08% - 100 eopchs, 98.02% - 1050 epochs, 98.02%
11/16 minibatch size 10, learning rate 2e-2, 100 epochs pretraining, repair every 5 minibatches, 6e-2 L1 scaling - 500 epochs, 98.65% - 550 epochs, 98.56% - 600 epochs, 98.57% - 700 epochs, 98.55% - 800 epochs, 98.59% - 900 epochs, 98.47% - 1000 epochs, 97.67% - 1050 eopchs, 98.42%
 11/16 minibatch size 10, learning rate 2e-2, 100 epochs pretraining, repair every 5 minibatches, 12e-2 L1 scaling - 500 epochs, 98.66% - 600 epochs, 98.65% - 650 epochs, 98.38% - 700 epochs, 98.62% - 750 epochs, 98.61% - 800 epochs, 98.57% - 900 epochs, 98.51% - 1000 epochs, 98.6% - 1050 epochs, 98.58%

Keep in mind that the hessian of a piecewise-linear network is always zero or undefined.  It seems likely that by breaking the connection between the encoding, decoding, and explaining-away matrices, steep valleys are created in the loss function landscape.  In general, these three matrices need to change in a coordinated manner, or the mismatch prevents effective sparse coding.  
Compared with 200 unit networks, 400 unit netwoks have L2 reconstruction loss that is 40% smaller, classification loss that is 60% smaller, and L1 loss that is slightly larger.  Of course, as a percentage, this implies that the half as many units are active.

11/19 400 fe units, 200 epochs pretraining, minibatch size 10, learning rate 4e-3, repair every 5 minibatches - a quarter of the units fail to train
11/19 400 fe units, 300 epochs pretraining, minibatch size 10, learning rate 2e-3, repair every 5 minibatches - average seems to be about 98.6%, maybe 98.55%
11/19 400 fe units, 200 epochs pretraining, minibatch size 10, learning rate 2e-3, repair every 5 minibatches, 4.5e-2 L1 sparsity - average seems to be about 98.7% - a third of the units remain noisy/poorly trained - L1 sparsity loss is 0.29, whereas with normal L1 sparsity, it is about 0.2
11/19 400 fe units, 200 epochs pretraining, minibatch size 10, learning rate 3e-3, repair every 5 minibatches - average seems to be about 98.75%, maybe 98.8% - maybe 10% or 15% remain noisy or untrained (compared to about 20% with learning rate 2e-3)

11/19 minibatch size 10, learning rate 2e-3, 100 epochs pretraining, repair every 5 minibatches - average seems to be about 98.4
11/19 minibatch size 10, learning rate 2e-3, 100 epochs pretraining, repair every 5 minibatches, ASGD, learning rate decay 10e-7 (DOES NOT AVERAGE until 4000 epochs) - average seems to be about 98.55%, but consistently hits 98.65% earlier in training
11/20 minibatch size 10, learning rate 10e-3, 100 epochs pretraining, repair every 5 minibatches, learning rates scaled down in proportion to the number of ISTA repeats - average seems to be about 98.65 early on, but decreases to 98.5% with more training - AFTER ONLY 100 EPOCHS OF PRETRAINING, ALL FILTERS ARE LEARNED WELL!!! - even with rescaling and 10 epochs of lower learning rate pretraining, doubling the learning rate to 20e-3 induces instabilities - it may be safe to scale the learning rate up to 10e-3 from 2e-3 since the learning rates of the encoding_feature_extraction_dictionary and explaining_away are scaled down, and these tend to be the most unstable; only decoding_feature_extraction_dictionary and classification_dictionary are actually subject to the increased learning rate - the network seems to overtrain after about 700 total epochs
11/20 minibatch size 10, learning rate 10e-3, 100 epochs pretraining, repair every 5 minibatches, learning rates scaled down in proportion to the number of ISTA repeats, classification loss scaled by 1/5 - average seems to be about 98.3% - Scaling down the classification loss clearly sacrifices the quality of the supervised classification training - if gradients are too large after the classification loss is introduced, the global learning rate should be scaled down, rather than reducing the classification loss - keep in mind that the ISTA rescaling reduces the learning rates, but does not change the scaling of the reconstruction losses
11/20 minibatch size 10, learning rate 2e-3, 100 epochs pretraining, repair every 5 minibatches, ISTA margin 1e-3 - average seems to be at least 98.7; maybe 98.72 (700 epochs, 98.68% on test set) - decreases to perhaps 98.65% towards 1000 epochs
11/20 minibatch size 10, learning rate 2e-3, 100 epochs pretraining, repair every 5 minibatches, ASGD, learning rate decay 10e-7 (proper ASGD averaging) - average seems to be about 98.6 before averaging at 1000 epochs, but has higher variance and higher max values earlier in training - 500 epochs averaged since 400, 98.8% (98.76% on test set) - 700 epochs averaged - 98.76% - 1050 epochs averaged since 400 - 98.74%

11/21 learning rate 10e-3 during pretraining, 10e-3 during supervised training, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats, ASGD, learning rate decay 10e-7 - average peaks at about 98.55% - averaged at 500 epochs, 98.57% - averaged at 700 epochs, 98.5% - averaged at 1000 epochs, 98.5% - WHY IS THIS SO MUCH LESS EFFECTIVE THAN THE PREVIOUS ASGD RUN?!?
11/21 learning rate 10e-3 during pretraining, 5e-3 during supervised training, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average peaks at 98.5% and decreases to 98.35% with further training 
11/20 (actually 11/21) learning rate 10e-3 during pretraining, 2e-3 during supervised training, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average about 98.35% 
 
11/21 400 fe units, learning rate 10e-3 during pretraining, 10e-3 during supervised training, 200 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average is about 98.75%, and remains consistent throughout training
11/21 400 fe units, learning rate 10e-3 during pretraining, 10e-3 during supervised training, 200 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats, ASGD, learning rate decay 10e-7 - never actually averages weights before job was killed - average performance of unaveraged weights is about 98.65%, which is less than in comparable non-ASGD runs; the difference is learning rate decay with smaller constant but also smaller exponent
11/21 400 fe units, learning rate 10e-3 during pretraining, 10e-3 during supervised training, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average peaks very quickly around 98.85% and decays to 98.7% - LOOK INTO THIS!  Performance peaks at 98.9% (250 epochs)

The density of full-digit units decreases substantially when the total number of units increases from 200 to 400
ASGD doesn't seem to be nearly as effective when the learning rate of the encoding_feature_extraction_dictionary and explaining_away are scaled down.  It's possible that with the larger learning rate decay 10e-7, learning simply isn't fast enough.
Now that the learning rate of the encoders is balanced with that of the decoder, consider increasing the number of ISTA iterations.  Previously, the efficacy of large number of ISTA iterations may have been reduced since it caused the encoder learning rates to scale up too much.
Doubling the learning rate, even after 20 epochs of pretraining (with associated learning rate decay of about 0.5) still induces instability initial instability, causes some units to learn whole digits, and increases the negative background on the encoding dictionary elements

It makes sense to bound the magnitude of the classification dictionary rows.  Otherwise, once all of the elements of the training set are classified correctly, the softmax can be saturated and learning brought to a standstill by simply scaling up the classification dictionary, even if the output of the classification dictionary is often ambiguous (i.e., at least two classes have similar magnitude).  Indeed, it seems as if good performance should be achievable with a relatively low bound on the magnitude of the classification dictionary rows.  In this case, large values of the log likelihood can only be achieved if the incorrect classes have small output.  However, it might be problematic if datapoints that are consistently classified correctly fail to saturate the softmax, and thus contribute a significant gradient, swamping the gradients due to a minority of truly ambiguous outputs.  On the other hand, the problem may simply be that the gradients of the classification dictionary are too large, resulting in unstable training; this should be resolvable by scaling down the classification gradients/step size.

Lower learning rates may lead to considerably worse performance because they allow the network to fall into local minima.  In particular, these local minima probably classify (almost) all of the elements of the training set correctly, but they are not robust, in that small perturbations of the parameters (and correspondingly of the input) lead to different classifications, either because the hidden state falls into a very different attractor, or because the output was ambiguous by amplified by scaling up the classification dictionary, thus leading to a much more certain classification after the softmax.  The latter condition should be addressable by bounding the magnitude of the classification dictionary rows during training.  For the latter condition, note that the encoder is continuous, although the derivative does not exist everywhere.  The gradient is dependent on the eigenvalues of the explaining away matrix.  

ELIMINATE FULL NORMALIZATION OF decoding_feature_extraction_matrix; ALLOW COMPLEX UNITS TO PRODUCE NO RECONSTRUCTION OUTPUT, since they are compatible with many renditions of a given digit, and so will innevitably introduce errors to the reconstruction.  This might allow more units to become complex, improving classification accuracy.

11/26 200 fe units, ASGD, learning rate 2e-3, learning rate decay 10e-7, 100 epochs pretraining, learning rates *NOT* scaled down in proportion to the number of ISTA repeats - replicate good 11/20 run - averaged performance (from 300) at 600 epochs, 98.61%; 750 epochs, 98.59% - nonaveraged performance is about 98.5% - WHY IS THIS WORSE THEN 11/20?!?  CODE APPEARS TO BE IDENTICAL!!!  Perhaps 11/20 was just an anomalously good run
11/26 200 fe units, ASGD, learning rate 10e-3, learning rate decay 5e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - averaged performance at 500 epochs, 98.6%; at 600, 98.5%; 750 epochs, 98.46% - typical performance, 98.5%
11/26 200 fe units, learning rate 10e-3 during initial pretraining, doubled after 20 epochs, 10e-3 during supervised training, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - typical performance 96% - explaining away is distorted by large negative values
11/26 200 fe units, 11 ista iterations, learning rate 5e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average performance, 98.67%, decaying to 98.6% with continued training - explaining away row norms are generally near 1, and bounded above by 1.5
11/26 200 fe units, classification dictionary bounded at 1 and gradient scaled down by 0.1, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average around 98.45% - classification dictionary rows have one large positive element a few negative elements; the rest are near zero - some explaining away rows have huge norms (>4)
11/26 200 fe units, classification dictionary bounded at 2.5, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average around 98.6%, decaying down to 98.55% with continued training - classification dictionary rows have one large positive element a few negative elements; the rest are near zero - some explaining away rows have huge norms (>4)
11/26 200 fe units, classification dictionary bounded at 5, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average around 98.68%, decaying down to 98.6% - classification dictionary rows have one large positive element a few negative elements; the rest are near zero - these single large positive elements correspond to the only digit-ID features, but there are more mid-level features than usual - some explaining away rows have huge norms (>4)
11/26 200 fe units, classification dictionary bounded at 10, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average seems to be about 98.6%, quickly decaying down to 98.45% - at 300 epochs, classification dictionaries are normal (not just one large positive element) and row magnitudes of explaining away are still relatively moderate (<3), but with additional training the classification dictionaries contract to have only a single large element per row, and explaining away row magnitudes exceed 3.5
11/26 200 fe units, decoding FE dict bounded above but not fixed, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - decoding feature extraction dictionary columns still all basically have maximum magnitude, so this doesn't seem to do much - the few columns that aren't maximal don't correlate with the complex units, which extract the digit class identity
11/26 400 fe units, ASGD, learning rate 10e-3, learning rate decay 5e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - 450 epochs (averaging since 400), 98.8%; 700 epochs, 98.66%; 900 epochs, 98.59% - nonaveraged performance is not especially stable, sometimes dropping below 98% even after 900 epochs of learning rate decay
11/26 800 fe units, learning rate 10e-3 during pretraining, 10e-3 during supervised training, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - 100 epochs does not appear to be sufficient pretraining; many units have poorly formed receptive fields - after hovering around 98.75% after 150 epochs, performance remains stably at 98.68% from 220 epochs to 430 epochs - about 20% of units do not have well-formed receptive fields - the classification dictionary is frighteningly sparse given the number of units it has to choose from 

Removing full normalization of decoding_feature_extraction_dictionary columns doesn't seem to do much, and can safely be incorporated into the network.  Increasing the number of ISTA iterations to 11 and introducing a mild bound of the classification dictionary row magnitudes both seem to help a little.

It seems as if the strong recurrence developed in the explaining-away rows for complex units, which respond to a whole digit class rather than a particular stroke, is dependent upon the details of the softmax applied to classification dictionary output.  If the classification dictionary output is subject to a nn.NormalizeTensor before the softmax, so constant scalings are factored out, then the explaining away recurrence is much weaker, although complex units with diffuse inputs still develop.  This variation is odd, since the sparse coding output is subject to a nn.NormalizeTable before being passed to the classification dictionary in both cases.  

Poor performance on 11/21 runs with reduced steps size during supervised training is associated with weaker recurrent explaining away connections.  

TRY COMBINING ASGD, 11 ISTA repeats, and normalization of the rows of the classification dictionary, along with 400 or 800 hidden units
WHY DOESN'T THE NETWORK LEARN TO USE *OR* IN THE TOP CLASSIFICATION LAYER EFFECTIVELY?!?

11/27 200 fe units, classification dictionary output normalized, decoding FE dict bounded above but not fixed, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average performance about 98% - explaining away rows remain small, and when only seem to add negative connections to produce digit ID units
11/27 200 fe units, classification dictionary output normalized and scaled up by 5, decoding FE dict bounded above but not fixed, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - average performance about 98.25% - looks like a more mild version of when output normalization is scaled to 1
11/27 200 fe units, ASGD, learning rate 10e-3, learning rate decay 20e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - nonaveraged performance decays from perhaps 98.55% to 98.45% - averaged performance is 500 epochs, 98.6%; 700 epochs, 98.6%; 900 epochs, 98.58%; 1050 epochs, 98.52%
11/27 (marked as 11/26) complete repeat of good 11/20 ASGD run - learning rate 2e-3, 100 epochs pretraining, repair every 5 minibatches, ASGD, learning rate decay 10e-7 - unaveraged performance stably plateaus at about 98.65% - averaged performance is 500 epochs, 98.75%; 700 epochs, 98.66%; 900 epochs, 98.7%; 1000 epochs, 98.7% - THIS IS STILL BETTER THAN EVERY OTHER ASGD RUN - WHAT IS THE DIFFERENCE?!? - complex/digit ID units are surprisingly dense, and the classification dictionary seems unusually non-sparse - NOTE that the training rate on the classification dictionary is effectively smaller on this run, since we don't scale up the learning rate to compensate for scaling down the training rate on the repeated ISTA matrices; this may tie into the problem of the classification dictionary scaling exploding, which presumably kills the gradient since all training dataset elements saturate
11/27 800 fe units, decoding FE dict bounded above but not fixed, learning rate 10e-3, **200** epochs pretraining (but marked as 100), learning rates scaled down in proportion to the number of ISTA repeats - performance around 98.8% after 300 epochs - about 20% of units still untrained despite the extended pretraining interval - most units are strokes; very few digit ID/mid-level/complex units (maybe 3%) - classification dictionary has fewer than 5 strong connections per digit class

11/28 400 fe units, 11 ista iterations, learning rate 5e-3, dec FE dict bounded above, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - after 250 epochs, mostly strokes; some digit ID classes seem to have only one complex unit, and not mid-level features - performance is stable around 98.78%


TRY DOING 11 ISTA REPEATS WITH THE MAX VALUE OF ELEMENTS IN THE CLASSIFICATION DICTIONARY BOUNDED
nn.NormalizeTensor WAS WRONG UP UNITL THIS POINT!!!  Runs that normalized the output of the classification dictionary were flawed!!!  It still seems sensible to normalize the output of the classification dictionary, rather than just bound its parameters, since the output of the sparse coding is already normalized!
TRY 11 ISTA REPEATS WITH BOUNDED CLASSIFICATION DICTIONARY ROWS - **MAKE SURE** that row magnitudes are merely bounded above, rather than fixed - Bound should probably match the row magnitudes observed when performance is best with unconstrained training; suggests that norm should be about 15 with 400 units.  Even with only 200 units, classification dictionary row norms increase quickly, and seem to saturate near 15 after only 100 epochs of supervised training, long before performance peaks
What if we just (or also) turn down the scaling on the classification dictionary training rate?

11/29 200 fe units, class dict mag bounded by 5, class dict grad scaled by 0.2, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - plateaus at about 98.68% - performance *does not* seem to decay with continued training - after 600/700/800 epochs, even given this realtively heavy regularization, the network only misclassifies 11 elements of the training set. - BEST RUN SO FAR ON 200! 601 epochs, test error 98.79% (may be anomalous)
11/29 200 fe units, class dict mag bounded by 5, class dict grad scaled by 0.5, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - plateaus at about 98.55% (strangely, this seems lower than the 11/26 run with the same bound on the classification dictionary row magnitudes)
11/29 200 fe units, class dict mag bounded by 5, 5 epochs classification pretraining, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - seems to speed initial training, but performance plateaus at only 98.55%
11/29 200 fe units, 11 ista iterations, class dict mag bounded by 5, learning rate 5e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - performance is not very stable; learning rate may be too large, plateaus at 98.5%
11/29 200 fe units, class dict units bounded by 3, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - classification DOES NOT become sparse; rather it is very dense! - filters are a normal mix of strokes and digit-ID features - average performance seems to be about 95.55%

How are bounds on row magnitudes of the encoding matrices related to a collective L2 weight-decay?
The issue of soft-max saturating due to the final classification matrix scaling up in magnitude should be a pervasive problem whenever softmax is used with negative log likelihood.  How is this addressed elsewhere?
Note that after performance plateaus, the networks only misclassify a small handful of elements from the training dataset.  This indicates that even with only 200 hidden units, the networks have more than enough power to represent the full dataset; the bias error is already zero, and all testing error comes from variance.  Adding units should only increase the variance error of the dataset without any benefit to the bias.  The solution is presumably more regularization, rather than more hidden units and the corresponding unnecessary representational power.  

Consider bounding explaining away matrix as well.  All of the bounding may be necessary to keep gradients from exploding as they propagate down the LISTA stack.  Note that the dynamics of backpropagation are very similar, although not identical, to the forward dynamics.  Backpropagating through a linear transform corresponds to taking the transpose of the associated matrix.  Backpropagating through a threshold-linear nonlinearity corresponds to only passing signals backwards if the input signal was passed forwards.  If the LISTA stack is sufficiently deep, the set of active units should remain constant for most of the stack, so forward and backward dynamics are similar if the explaining-away matrix is symmetric (but what about the encoder's contribution?).  

11/30 200 fe units, weight decay 0.5e-3, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats - accuracy is never better than 98.35%, and often much worse - encoding rows seem to have the large negative regions characeristic of discriminative training without unsupervised pretraining - classification dictionary rows are maximally sparse, and only have one positive element - the other weight decay runs are qualitatively similar

THE REASON THAT THE GRADIENT MAGNITUDES ON THE CLASSIFICATION DICTIONARY ARE SO MUCH LARGER THAN THOSE ON THE ISTA MATRICES IS THAT THERE IS AN EXTRA NORMALIZATION STEP BETWEEN ISTA AND THE CLASSIFICATION!!!  REMOVE THIS!!!
It is probably the case that, without normalization after ISTA, the outputs are too small to significantly saturate the softmax.  The explaining-away matrix thus must grow very large to produce an effective output, which disrupts the learning of good discriminative parts.  It actually probably makes sense to discard the magnitude of ISTA, especially if it's actually doing part-based reconstruction, in which case the magnitude is determined by the magnitude of the input, and the normalization on the decoding dictionary columns.  Digit ID outputs are free to grow large (subject to the L1 regularization) without affecting the reconstruction (so long as the dictionary columns are only bounded above) even if the outputs are normalized; this just sparsifies the normalized output.
Even with normalization of ISTA outputs, the rows of the explaining-away matrix corresponding to the digit ID units grow large.  

Learning_rate_decay is generally 5e-7 up until this point, although in some descriptions I marked it as 10e-7

12/4 200 fe units, SoftClassNLLCriterion, class dict mag boudned by 5, class dict grad scaled by 0.2, learning rate 10e-3, learning rate decay 5e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - average performance is about 98.58% - presumably because of the bound on the classification dictionary row norms, the logsoftmax output is not much flatter than normal (about -4.3) - moreover, it's not surprising that the anomalous pressure to flatten the output corresponding to the incorrect classes would hurt performance - classification dictionary rows are still very sparse, although not perfectly so, and ISTA matrices look similar to their usual form - only miss 9 elements on the training set, so difference in performance is a matter of regularization
12/4 200 fe units, SoftClassNLLCriterion, class dict mag boudned by 5, class dict grad scaled by 0.2, learning rate 10e-3, learning rate decay 10e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - average performance 98.5% - logsoftmax output has higher variance than with lower learning rate decay

12/4 200 fe units, manual explaining away diag, ClassNLLCriterion, class dict mag boudned by 5, class dict grad scaled by 0.2, learning rate 5e-3, learning rate decay 10e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - average performance is about 98.68%, and seems to match the complementary run on 11/29 (which had an explicit diagonal in the explaining away matrix)
12/4 200 fe units, manual explaining away diag, ClassNLLCriterion, class dict mag bounded by 5, class dict grad scaled by 0.2, learning rate 10e-3, learning rate decay 10e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - average performance is about 98.6%; maybe a little higher

Increasing the learning rate decay from 5e-7 to 10e-7 seems to hurt performance, at least with 200 units and learning rate 10e-3

12/5 200 fe units, explaining away rows normed like encoding FE rows, manual explaining away diag, ClassNLLCriterion, class dict mag boudned by 5, class dict grad scaled by 0.2, learning rate 10e-3, learning rate decay 5e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - Even the training set is not learned perfectly - average perforamance 97.4% (horrible!) - encoding matrix is much more discriminative than usual - digit class units do not fully develop - shrink dictionary changes little between layers - classification dictionary is dense
12/5 200 fe units, explaining away rows normed with bound scaled by 3, manual explaining away diag, ClassNLLCriterion, class dict mag boudned by 5, class dict grad scaled by 0.2, learning rate 10e-3, learning rate decay 5e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - average performance about 98.55% - digit ID units are present, but while the explaining away matrix row magnitudes are not uniformly saturated, they are all within a factor of 2 - classification dictionary has no more than two large elements per row - shrink dictionary elements refine continuously through the layers - encoding matrix is fuzzier than usual - misses 25 on the training set, so not perfect, but not bad - this form of regularization just doesn't seem to be especially helpful
12/5 weight decay 1e-4, 200 fe units, manual explaining away diag, ClassNLLCriterion, class dict mag boudned by 5, class dict grad scaled by 0.2, learning rate 10e-3, learning rate decay 5e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - average performance 98.4% - maximum exp row norm 1.8 (compared to 3.9 without weight decay) - other matrices seem to be only constrained by the hard bounds put on row and column magnitude - misses 21 on training set
12/5 hinge loss 0.8, 200 fe units, manual explaining away diag, HingeClassNLLCriterion, class dict mag bounded by 5, class dict grad scaled by 0.2, learning rate 10e-3, learning rate decay 5e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - average performance is maybe 98.6%, but not very stable - only 2 units are not active at the top-most ISTA stage - otherwise looks similar to non-hinge loss - maximum explaining away row norm is still over 4 - misses 16 on training set
12/5 11 ista iterations, 200 fe units, manual explaining away diag, ClassNLLCriterion, class dict mag bounded by 5, class dict grad scaled by 0.2, learning rate 5e-3, learning rate decay 5e-7, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA - average performance is maybe 98.65% - eventually decays to 98.55%

Runs with more units are MUCH SPARSER.  This may explain why all units do not train with 800 units.  
CIFAR SEEMS TO WORK AS IS!!!  It might be necessary to scale the learning rate down by 10.  Check to see if that helps or not before firing off a long run.




Consider adding in -(1/s)*tanh(s*x) to FIxedShrink to make it continuous
Instabilities in validation error late in training correspond to instabilities in the reconstruction loss on the training set; it seems as if the training procedure is actually unstable, as opposed to the variation in validation loss being a result of the fact that the validation set is distinct from the training set.
Sparsifying loss seems to consistently remain at about 0.2




All techniques to keep receptive fields well-formed, including scaling down repeated ista modules, pretraining the classification dictionary, and scaling down the classification loss while scaling up the learning rate; do not seem to improve performance.  However, I don't think I've tried scaling down the classification loss and scaling up learning rate in the absence of classification pretraining



Consider the possibility that this works so well because, with normalized rows of encoding_feature_extraction_dictionary, all units contribute; none are ignored.  Without this normalization, the ratio of magnitudes starts at around 4 after pretraining, and grows to around 8 once the classification loss is activated.  
It seems as if, once we start training witht he classification loss, the feature encoding dynamics move out of the (group-)sparse reconstruction regime.  Presumably, this is due to the pressure to increase the magnitude of the output.  Encoding pooling dictionary magnitudes also shoot up.  Note though that this occurs even though the output is normalized before classification.
The problem may be the max operation which the L2 norm roughly performs.  Only the largest input matters, and so it is trained disproportionately.  If the loss function benefits from increasing the magnitude of the output, this will develop into a positive feedback loop, in contrast to the normal operation of group sparsity, which favors all elements of a group being activated equally.  The difference is whether the unit activities or the pooled activities are used as the output.  But why would large outputs be desirable, given that outputs are normalized before the classification layer?  This may be the only way to silence units corresponding to the wrong classes, which otherwise will tend to be at least weakly active.  We go from penalizing the group sparsity to rewarding it!  Large group sparsity corresponds to large output to the next layer.  We can reestablish a penalty rather than a reward on group sparsity by shifting the balance between the L1 group sparsity penalty and the classification loss away from the classification loss.  
This is consistent with the behavior observed when the sign of the ClassNLLCriterion is flipped.

A certain magnitude output is necessary to satisfy the classification loss.  If this is not immediately achieved using the classification dictionary, the increase in output magnitude will be effected by increasing outputs throughout the processing chain.  We may be able to avoid the corruption of the sparse coding layer by initializing the classification dictionary to be huge.  Alternatively, pretrain the classification dictionary alone before training everything together.  After the sparse coding layer is corrupted to produce larger outputs, it is hard to learn the original correct dynamics, since the classification dictionary needs to be increased in register with decrease in output magnitude from the sparse coding layer.  After the classification dictionary is trained by itself to set the row magnitudes to a reasonable range, training of the full network at the usual rate seems to be unstable, and leads to the corruption of the sparse coding stage.  It seems to be necessary to use a very small global learning rate immediately after learning is enabled in the entire network, and keep the learning rate low even after the initial burn-in.  Keep in mind that larger values of the classification dictionary will lead to larger backpropagated gradients in the rest of the network.

Consider the possibility that superior performance with normalized encoding_feature_extraction_dictionary rows is due not only to lack of corruption of the sparse coding layer once the classification loss is introduced, but also the learning of better features during the initial unsupervised pretraining.  Normalized rows may ensure that all features are used evenly, leading to the development of a more even tiling of features.  Using the resulting features to initialize supervised training, even without normalized encoder rows, may inherit this improved performance.  Supervised training with normalized encoding_feature_extraction_dictionary rows seems to lead to more even features than unnormalized rows.  

Try decreasing the number of ISTA iterations to one.
Try initializing a run with unnormalized encoding_feature_extraction_dictionary rows with the result of unsupervised trained with normalized rows; this will disambiguate the effect of normalization during pretraining from normalization during supervised fine-tuning.  

Merely normalizing the rows of the encoding_feature_extraction_dictionary does not ensure that the sparse coding layer will remain uncorrupted by the classification loss.  When used with classification loss pretraining and full learning rate, the encoder receptive fields grow large.  It seems as if this is mostly a result of the learning rate being too large.  The encoder changes extremely rapidly, and classification loss doubles.  This is surprising, since the gradient magnitudes aren't any larger than they are during unsupervised pretraining.  There seem to be occasional inputs that yield very poor classifications, since all the gradients on the feedforward pathway increase by a factor of about 10.  We don't want to turn down the training due to the classification loss too much, or classification trains very slowly.  At the same time, a large classification-induced gradient tends to disrupt sparse coding/reconstruction, since the classification gradient consists of a few large instances when inputs are classified incorrectly, interspersed amidst many small instances.  Try scaling down the classification loss and complementarily scaling up the learning rate, so that the reconstruction loss is effectively increased, and can quickly correct disruptions induced by large corrections to incorrect classifications.

It probably makes sense that classification pretraining leads to poor final performance.  During the pretraining, the network will lock itself into a sub-optimal local minimum.  The large classification dictionary weights learned during pretraining will fix the interpretation of each pooling unit, so their overall mapping cannot be changed afterwards (indeed, the classification dictionary is basically static after pretraining).  However, it might be profitable to have a "classification pretraining" interval during which the scaling factor on the classification loss is 0.1 or 0.2, before increasing it to 1.  This might allow the classification dictionary to gradually change in concert with the reconstruction loss, rather than performing alternating minimization between the two.  The 10/25 run seemed to use no more than half of the pooling units in constructing the classification.  The 10/11 run seemed to do better, but that may be an artifact of its early termination.  

The real issue is to keep the reconstruction gradients comparable to the classification gradients.  Whenever the classification gradients dominate learning, the sparse coding layer degenerates.  Suddenly introducing the classification loss will generally induce very large classification gradients, which will tend to disrupt existing learning.  Pretraining the classification dictionary reduces the magnitude of the classification gradients, at the expense of reducing the coordination between the feature extraction and the classification stages.  

It seems likely that the real problem is the nonlinearity of the encoder.  With six iterations of ISTA, the encoder is highly nonlinear.  Even when used purely for sparse coding, and ISTA encoder can exhibit rapid changes in output code in response to small changes in the input.  This instability is probably coopted by the classification loss; so long as each element of the training set induces a sufficiently different code, it's easy to train the classification dictionary to produce the correct output.  Training with the classification loss probably increases the instability of the deep ISTA encoder, reducing the loss on the training set, but failing to generalize to the test set.  Normalizing the encoding_feature_extraction_dictionary rows may reduce this tendency by making it harder to drive the ISTA encoder to instability.  Note though that increasing the magnitude of a row of the encoder_feature_extraction_dictionary is very similar to increasing the corresponding element on the diagonal of the explaining_away matrix.  Note further that the gradient of a row of the encoder_feature_extraction_dictionary will tend to be parallel to its current value, since in a sparse coder, the unit driven by a row will tend to be active only when the input is aligned with the current value of the row, and the gradient is proportional to the input.  

Incorrect classifications induce massive gradients.  This suggests that the classification loss really should be smaller.  However, I've observed massive gradients even with a classification loss scaling of 0.05.  
Based upon restarted runs, it seems as if, regardless of the global learning rate, the backgrounds of receptive fields are unstable until the traning set classification rate draws very near to 100%.  Before this, massive gradients induced by incorrect or ambiguous classifications perturb the receptive fields.  This seems like an unreasonable setting for learning.  What if we scale the classification loss down by 0.2, but with normalization of the encoding_feature_extraction_dictionary rows and without classification pretraining?  I've never tried this combination before.

It's probably important the the unsupervised pretraining not be disrupted by the initial bout of supervised training.  I suspect that reduced learning rates/classification loss scaling will only help if present from the beginning of supervised training.  It's easy to satisfy the supervised classification objective; it's important to retain the regularization implicit in the supervised pretraining to ensure generalization.  This would explain why restart runs that failed to properly set the learning rate decay performed so poorly.
Redo untrained pooling test runs with normalized encoding_feature_extraction dictionary

Units with seemingly disordered encoding_feature_extraction_dictionary receptive fields, and with large explaining_away fields, may actually be primarily using explaining_away to generate their encodings.  These units likley aggregate over many other sparse coding units.  That is, they're probably linear combinations of the highly nonlinear sparse coding units.  Projective fields based upon the encoding_feature_extraction_dictionary are largely similar to the encoding_feature_extraction_dictionary, but tend to have more prominent inhibitory regions.   These units only seem to appear after we start classification training, suggesting that a linear combination of sparse coding units is more useful than a L2 norm, which is closer to a max.  However, further note that many of these connections are negative, and the linear combination is passed through a shrink before passing onto pooling, so these units could be implementing AND-NOT operations, which are not possible with the non-negative encoding_pooling_dictionary.  Before classification training (using just unsupervised pretraining) the projective fields of explaining_away look like contrast-normalized versions of the encoding_feature_extraction_dictionary fields.  

L2 norm grows linearly with error.  Log likelihood with softmax grows as?!?  Does the extra normalization before the classification dictionary cause problems?
It seems plausible that the gradients really should be 100 times larger on incorrectly classified inputs as on correctly classified ones.  In the absence of an unsupervised reconstruction loss, the gradient should be due almost exclusively to misclassified inputs.  While the classification gradient must be balanced against the unsupervised regularizer, given that only a tiny fraction of the inputs are misclassified after the first few dozen epochs of supervised training, the classification gradient generated by misclassified inputs must be many times larger than the average reconstruction gradient.  However, the difference in gradient magnitude between feature extraction and pooling remains troubling.  But remember that we square the pooling weight matrix before using it.  ONLY the encoding pooling dictionary gradient is small; the decoding pooling dictionary gradient is as large as the gradients of the feature extraction dictionaries, suggesting that the problem really is the fact that the encoding pooling dictionary is squared.

Supervised classification training seems to make the encoding pooling dictionary sparser.  Moreover, after supervised classification training, the encoding pooling dictionary seems to connect disproportionately strongly to sparse coding units that reconstruct whole digits, via diffuse encoding dictionary rows and strong, widespread explaining away connections.  Nevertheless, the average pooling unit receives projections from more than two sparse coding units, and each row of the classification dictionary tends to have multiple strongly positive connections.  This suggests that, classification depends on highly nonlinear and-not operations, but that these sparse features are indeed pooled to produce the classification output.  

Deep convex network seem to achieve ridiculously good performance by training each hidden layer on the final classification task, and only passing this provisional classification onto the next layer (along with the original input).  They make a big deal of the fact that the top-level linear classifier can be optimized in closed form assuming that the first-layer weights are fixed; really, this seems similar to a (linear) SVM.  It's surprising that feeding a provisional classification (alone) into the next layer of a deep architecture is so effective, given that a linear combination of the provisional classification with the original input doesn't seem like an especially useful computation.  Has anyone tried training a deep (convolutional) neural network by adding in a final linear classifier (with associated loss) at each layer of the hierarchy?  Note that feeding the original input into each layer is similar to the operation performed by LISTA.  However, LISTA only applies the classification loss to the final output, and passes an arbitrary linear transformation of the hidden state onto the next layer, rather than the provisional classification alone.  Consider adding a classifier/classification loss to each ISTA iteration of LISTA.

The major contribution of techniques like deep belief networks is a method to effectively find the good local minima in a highly parameterized network.  Sufficiently powerful network architectures have been used for decades, but they easily fall into local minima.  LISTA may be disproportionately powerful, even with very few units, because the weight-sharing across layers ensures that gradient signals are always used efficiently.  If the network is too deep, and the initial layers of hidden units carry little information, we can (conceptually) chop off all but the last few layers, and the network effectively becomes much more shallow.  The original input is provided to all layers, so it cannot be lost due to poor initialization of the early layers.  Moreover, the same weights that yield effective sparse coding in the last layers are also effective in the first layers, so it is easy to bootstrap training of all layers based upon the last few layers.  Given sparse coding features (which are conceptually an AND operation), the sort of OR operation we want to perform is probably of the form "at least n."  That is, we don't want to know if any of m constituent features of a digit are present.  Rather, we want to flexibly compose smaller features into larger conglomerations, consisting of multiple component parts; it isn't safe to do OR and AND completely separately, since e.g. two parts that could each separately compose the top bars of a 4 can also effectively form a 9.  Rather, since sparse parts are mutually exclusive, we want to know if at least n parts are present from a group which consists of n segments of a figure.  Multiple parts corresponding to the same segment will not be present, since they are mutually exclusive.  

Aside from the denominator, Gated Softmax Classification by Memisevic, Hinton, et al. is basically a one-hidden-layer neural network with a soft rectified linear nonlinearity, if we look at log-likelihood and ignore the common log partition function.  log(1 + e^x) is the softplus approximation to threshold-linear/rectified-linear used by Nair and Hinton's Deep sparse rectifier neural networks.  The weights from the hidden units to the output units are then effectively disjoint groups of ones; alternatively, each output unit can be though of as having an independent group of hidden units.  Given the similarity to threshold-linear units and thus LISTA, consider taking the output of a LISTA network as threshold-linear after the classification dictionary, minus log of the sum of exp of classification dictionary outputs.  That is, treat the threshold-linear output as the energy, and construct a full log likelihood by adding back in the log partition function.  THIS IS EXACTLY WHAT nn.LogSoftMax combined with ClassNLLCriterion does!!!



Look at averaging SGD - Leon Bottou - do SGD as usual, but then final result is average of last few iterations
Minibatches of size 10 has a good chance of all being different classes; don't need learning rate much
Chaituh (first name) in Eero's lab, just graduated - represent data manifold as point plus gradient
Yann thinks number of epochs used for pretraining is larger than expected.  Try turning up learning rate during pretraining?  What happens if we only use 50 epochs of pretraining?  Keep in mind that the Jacobian of active units never goes to zero, since threshold-linear units that are above the threshold have a constant gradient, unlike sigmoidal units that have a decreasing derivative as they move away from the linear region and begin to saturate on either side.  

Geoff's trick for avoiding early overconvergence: bound L2 norm of all weights.  I'm already basically doing this; L2 norm of encoder rows is bounded, L2 norm of decoder columns is fixed.
Ask about deep convex nets.  What is a threshold for success?



So long as the set of active units doesn't change, the L1 sparsifying loss is applied at every layer through a weight that is equal to the bias on the rectified linear units, and the explaining-away matrix is symmetric, forward propagation is the same as backwards propagation, but backwards propagation has inputs from the classification units as well.  It is key that reconstruction be part of the loss function, so the pixel inputs are present during backwards propagation.




rose1 - 12/7 800 fe units, sparsity scaled down by 0.75, decoding FE dict bounded above but not fixed, learning rate 10e-3, class dict mag bounded by 5, class dict grad scaled by 0.2, **100** epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats
rose2 - 12/7 800 fe units, sparsity scaled down by 0.5, decoding FE dict bounded above but not fixed, learning rate 10e-3, class dict mag bounded by 5, class dict grad scaled by 0.2, **100** epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats
rose3 - 
rose4 - 12/7 800 fe units, sparsity scaled down by 0.75, decoding FE dict bounded above but not fixed, learning rate 10e-3, class dict mag bounded by 5, class dict grad scaled by 0.2, **200** epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats
rose6 - 
othello - 
duncan - 
iago - 
macbeth - 
banquo - 
ceres - 
iris - 
cassio - 

11/29 200 fe units, class dict units bounded by 3, learning rate 10e-3, 100 epochs pretraining, learning rates scaled down in proportion to the number of ISTA repeats

cassio - ceded to Nathan







L2 regularization on classification matrix is like max-margin in SVM
Alternatively, use 1 - x/n, x, x, x, ..., which is not maximized by scaling up the classification dictionary arbitrarily; this may render regularization of the encoding matrix unnecessary, since there is no longer pressure to make the outputs blow up
Alternatively, clip the loss function; if probability out of the softmax is above some threshold, loss gradient is zero.  Can even just skip backprop entirely for samples that pass the threshold.  Basically makes the loss even more hinge-like
To normalize CIFAR, subtract mean on a per-pixel basis (equivalent to a bias unit) and set standard deviation to one for each color channel separately (less variance in the blue channel)
Try to visualize how the template units develop their output.  Maybe show reconstructions from each layer as you move up the stack.
Try doing zero and one ISTA iteration for comparison.
Try decreasing the number of hidden units to see how performance degrades.
  
