Overall concept:

Focus on structure learning.  Like the group sparsity papers, we want to show that a simple training algorithm leads to the emergence of biologically suggestive structure that is present in the data, rather than the algorithm itself.  This frees us from having to beat state-of-the-art performance, so we can focus on smaller networks.  
Switch to 11 ISTA iterations to allow emergence of more structural tiers.  
Can get a sense of hierarchy by looking at when a unit turns on.  Maybe cluster activation trajectories?  Stroke units quickly plateau; categorical units grow monotonically?  Empirically, stroke units tend to turn on immediately, whereas categorical units generally wait an iteration before turning on.  A unit that waited two iterations before turning on would probably be on a third layer of the hierarchy.  
***Add small L1 loss on weights to make units beyond the first layer of the hierarchy have no encoder input, which clearly identifies them.  - This doesn't seem to work!  While the categorical units may have no activation on the first iteration, even a disruptively strong L1 weight regularization fails to set their encoder weights to zero.
Show that performance is bad with too few ISTA iterations, which doesn't admit a hierarchy.  Try reducing the number of layers in the network one by one, using the weights from the larger network as a seed?



Figures:

*Categoricalness is measured by the angle between the encoder and the decoder: dot product of vectors divided by the product of their magnitudes
*Dictionaries reconstructed for early, middle, and late layer - note that this is identical to the encoding matrix if we're doing ISTA, and so better reflect the meaning of the unit than the optimal linear regressor from the input to unit activity (the optimal linear encoding matrix), which will be distorted by correlations in the data.
*Histogram of explaining away row magnitudes
*Probability of activation on the first ISTA iteration, given that the unit is active on some iterations, versus categoricalness (or histograms of row magnitudes separated by whether first activation is 0 or 1, if there is no variation in first activation iteration)
*Average recurrent connection strength versus categoricalness
*Recurrent connection magnitude versus categoricalness
*Weighted average angle between decoding dictionary column and decoding dictionary columns of units to which the unit is connected by the explaining away matrix, for both positive and negative connections
*Use the angle between encoder and decoder as an explicit measure of categoricalness, and compute the weighted-average-categoricalness of the input to each unit, and plot against the units own categoricalness to show that part-units only receive input from other part-units, whereas categorical-units receive input from both part-units and categorical-units.  It will probably be important to ignore the diagonal in this calculation, or it will skew the results.
*Decoding dictionary cols for units to which a given categorical unit is strongly connected (substantiate claim of pooling over similar units)
*Decoding dictionary cols for units to which a given stroke unit is strongly connected (substantiate claim of anti-pooling)
*Angle between encoder and decoder versus percentage of inputs for which unit is active at the end of processing (categorical units turn on no more than 10\% of the time, whereas part units are less sparse)
*Max classifier dictionary connection versus some measure of categoricalness (e.g. recurrent connection magnitude) - show that the logistic classifier primarily uses the categorical units
*Mean final activity when activated versus categoricalness - categorical units are much more active than part units, since as shown in the next figure, the decomposition used is template-plus-perturbation, so most of the energy of the input is accounted for by the templates of the categorical units, requiring little further activation
*Largest parts used to reconstruct an input - shows that the strategy used is template-plus-correction, as in Simoncelli's work
*Decoding column mean versus categoricalness - if part units are perturbations to a template, the templates should be strongly positive, reflecting the non-negative nature of the input, whereas the perturbations should be closer to mean-zero, to correct rather than merely add to the template - indeed, categoricalness is positively correlated with decoding column mean after discriminative training; before discriminative training, almost all units have positive decoding column mean
*Encoding column mean versus categoricalness - all units (including categorical) have approximately mean-zero encoding rows, consistent with both the mean-zero decoding rows of parts-units (since their encoders and decoders are aligned by definition), and with the tendency of categorical units to remain off during the first ISTA iteration
*Ratio between explaining-away weights and the dot-product between decoders (minus the identity matrix) versus categoricalness of the recipient (also versus categoricalness of the source) - shows that part units (for which the decoder is aligned with the encoder, as required for ISTA) also have ISTA-consistent explaining away connections by and large, although there are some significant deviations even for the larger entries of the explaining-away matrix.  It is not yet clear whether part units have the relevant ISTA-specified explaining-away connections to categorical units in particular.  Categorical units definitely do not have uniformly ISTA-consistent explaining away inputs, given their strong negative connections from other categorical units.  However, they also appear to lack ISTA-consistent connections from part-units (and themselves).  Note that if part-units are used to select the categorical units activated, which seems necessary given the unstructured encoder rows of categorical units, then the connections from part-units to categorical units should be the additive inverse of the ISTA-consistent connection, stimulating rather than inhibiting matching categorical units.  Categorical units would then accumulate evidence like cells in MT (do complex cells in V1 do something similar?).  

*** variance of unit when activated
Are categorical-unit connections to part-units generally negative?  Are part-unit connections to categorical units generally positive?
Are the explaining-away connections to part-units that do not match the ISTA value mostly large negative connections from categorical units, which turn off the part unit if a particular category is activated?
*** Plot weighted angle between decoders, separated by both the sign of the connection and categoricalness of the source (create a sufficiently large dead region so we can be sure of part-units versus categorical units).  Expect weighted angle to decrease robustly as connections grow larger for inputs from parts units
Expect 

Expect inputs from parts-units with small angle between decoders to switch in sign as the recipient unit becomes more categorical.  Also expect parts-units with small angle to be the strongest connections in both cases.  Don't split into positive and negative connections; report weighted angle of input.  But then it's impossible to distinguish between strong connections with small angle, and weak connections with large angles.  Rather than angle, use (pi - angle)
Expect signed-weighted (pi/2 - angle) from part-units to go from negative to positive as the categoricalness of the destination increases; use pi/2 as flipping point, since we expect phase-flipped units to have negative connections
Expect signed-weighted (pi/4 - angle) from categorical-units to go from negative to positive as the categoricalness of the destination increases (initially, negative connections from similar templates; later, negative connections from dissimilar templates); use pi/4 as flipping point, since phase-inverse doesn't mean much for categorical units, and orthogonal units are maximally dissimilar

For part-units, cos(angle) between decoders (equal to dot product) versus connection weight, separately for connections from part-units and categorical-units.  Basically follows a straight line in both cases, showing that part-units basically perform ISTA, since these are the requisite ISTA weights.  I've also repeated these plots for categorical-units, but they don't have a strong regressor line, and so are difficult to interpret, although the part-to-categorical connection seems to have a V-shaped dip around 0, leading to the next plot.

Average connection weight from part to categorical unit with a given dot product between the decoders versus dot product between the decoders - has a prominent dip around 0, showing that connections from part-units to categorical-units tend to be positive if they are aligned or anti-aligned, and negative if they are unaligned.  This is consistent with an energy-based model of complex cells, which similarly pool over shifted copies of the same filter; i.e. aligned and anti-aligned.

cos(angle) between classifiers from categorical to categorical versus connection weight - shows that categorical units tend to be positively connected if they represent the same class, and negatively connected if they represent different classes.  

%Really, what we want to know is, for each value of the angle between decoders, what is the average weight of the connections.  Plot, for part-unit and categorical-unit receivers separately, cos(angle) between decoders versus connection weight.  cos(angle) = dot product between decoders, since decoder cols are normalized to have magnitude 1, so we expect this to be a straight line for the part-units, since they learn ISTA-weights.  For categorical units, separate part-sources and categorical-sources.  Part connections to categorical units should have decreasing weight as angle increases, going from positive to slightly negative.  Categorical connections to categorical units should have decreasing weight as angle increases, going from slightly positive to very negative.  However, there is no reason to believe that the crossing point for these two cases is the same, making it difficult to combine them into a single weighted-angle number for each unit.  KEEP IN MIND that a mean-zero, high-frequency part decoder may have zero dot product with a positive-mean, low-frequency categorical decoder, even if the two are aligned.  It's probably sensible to expect that the weights of categorical-to-categorical connections with positive decoder dot-product should have high variance and nearly zero mean; we expect positive connections between same-category units, but the most negative connections between different-category units with similar decoding columns.  


Figures to do:


***Max (over digit ID) of correlation coefficient of unit activity with digit ID - show that categorical units are actually categorical
***Max (over digit ID) of angle between decoder and average digit
***Trajectory of angle between encoding matrix and optimal decoding dictionary over time/iterations
***Angle between optimal decoder for each layer and actual decoding dictionary; maybe difference between first and last such angle
REPEAT ALL PLOTS FOR NETWORK BEFORE SUPERVISED FINE-TUNING to show structural change induced by discriminative training; consider putting both sets of points on one plot but with different colors to emphasize the difference


Figures to consider:

Optimal encoding dictionary?
Backpropagate to find optimal stimulus (but this is subject to weak correlations in the data.  perhaps find the input that comes closest to only activating the selected unit)
Plot trajectories of layer activities (across layers) in PCA or embedded space
Average explaining away input versus encoder input at the last layer ISTA layer?  Use the weighted sum of input norms (take absolute value of unit activities before multiplying by explaining away matrix), rather than the actual input (explaining away matrix times signed unit activities), so it is apparent when excitation and inhibition are large but balanced.

Modification to try:
Try using higher learning rate for unsupervised pretraining, particularly with 800 hidden units, and then reverting back to the standard rate for discriminative fine-tuning
Try unhooking the repeated explaining away matrices, to demonstrate that weight-sharing helps deep learning
Try using two ista iterations
*Try turning off L1 sparsifying loss for units for which the L1 loss passes below some threshold.  Once the reconstruction loss passes below a critical threshold, the gradient of the L2 reconstruction loss will be less than that of the L1 sparsifying loss.  Receptive fields seem to be learned progressively.  Once the reconstruction loss is sufficiently low, turning on a poorly trained unit will increase the loss more than its contribution to reconstruciton decreases the loss, and untrained units should simply be turned off completely.  We need all units to learn to be more sparse and accurate, but this can only occur if the untrained units be bootstrapped to a sensible representation.  
Note that even incorrectly classified inputs, with prominent template activations, are reconstructed almost perfectly.  This suggests that the learned representations are insufficiently sparse (with 400 units) since an incorrect template can be perturbed until it looks like a different input class.  If fewer perturbations were made, restricting the data manifold to a lower-dimensional space, it should be harder to fold from the region associated with one template to that associated with another.


Introduction:

Like Babel presentation: deep networks are powerful, but depend upon slow and heuristic pretraining.  Sparse coding is effective but depends upon slow, iterative encoders.  We can marry these two techniques to potentially combine their strengths while eliminating their weaknesses.


Main point figures:

Input is decomposed into a template and perturbation.  The same template is used for very different inputs, since the space of perturbations is rich enough to encompass diverse transformations
  - For selected inputs (choose a few that are very different but from the same class; particularly 4's and 8's, for which many hidden units have strong classification dictionary entries), show the gradual reconstruction of the input, adding in an ever larger number of parts (from largest to smallest contribution).  Below that, show the gradual reduction of the residual input (minus the part that's already been reconstructed).  This shows whether the classification units take out the entire template, or only a small fraction of it.  Below that, show the parts that are being added, possibly scaled by their contribution magnitude (otherwise, show this magnitude e.g. via the length of a bar)
  - Histograms of mean decoder column weight, separated by part/classification unit - shows that classfication units are net-positive, whereas part units are mean-zero perturbations
Part units do sparse coding on the residual input after the classification-unit template is subtracted out
  - For part units (defined by classification dictionary column norm), show connected decoders sorted by explaining-away connection magnitude
  - Show connection weight versus ISTA ideal
Classification units are excited by aligned part units and inhibited by orthogonal part units; part units are compatible with certain class IDs and not with others; a single part unit generally activates all compatible classification units; there is a hierarchy of classification units; almost every part unit projects to the most template-like classification units
  - For part units (defined by classification dictionary column norm), show connected classification-unit decoders sorted by explaining-away connection magnitude and class ID
  - V plot: average explaining-away connection weight, binned by alignment between decoders, for connections from part-units to categorical units (defined by classification dictionary column norm)
Classification units enforce a winner-take-all choice over input class
  - For classification units (defined by classification dictionary column norm), show connected classification-unit decoders sorted by explaining-away connection magnitude, separated by sign of connection 
  - Histogram of connection magnitudes between classification units, with separate overlaid histograms for connections between units with positive/negative dot products between classification dictionary columns
Part units are driven directly by the input; classification units are driven primarily by part units
  - Histogram of angle between encoder and decoder, with separate overlaid histograms for part units and classification units defined by classification dictionary column norm
  - Overlaid trajectories of units in response to a single input, with lines colored by classification dictionary column norm; show that part units turn on first and plateau, while classification units turn on later and grow without bound.  Since the part-units that are most diagnostic of a categorical unit will likely be turned off as the categorical unit increases its activation and subtracts out those features from the input, it makes sense to integrate part-unit excitations to categorical units over time.
  - Scatterplot of angle between decoder and encoder / probability of activating on the second iteration versus classification dictionary column norm
  - Scatterplot of weighted average categoricalness of recurrent connections versus categoricalness
The classification/part unit distinction has a number of other manifestations
  - Scatterplot of classification dictionary column norm versus: norm of explaining away dictionary row, average final value when activated


Main points:

Recurrence is one of the dominant structural features of the cortex.  In contrast, the machine learning community has found that deep networks can be extremely powerful, but has not realized much benefit from recurrence.  We unroll a recurrent network into a hierarchical network, but discriminative training induces hierarchical structure within the recurrent network.  A recurrent network is simply a hierarchical network with shared weights.  Moreover, sharing weights is not very burdensome if they are subject to similar gradients (which is the case in a LISTA network if all layers are subject to the L1 and L2 losses (only exact if network hits a fixed point)).  

Interpretable mid-level and digit-ID features arise spontaneously with discriminative training in a deep (but thin) LISTA network.  Discriminative fine-tuning does not (just) discover ``parts'' that are more discriminative; it constructs categorical units which have strong, predominantly negative recurrent connections and encoders that are not aligned with their decoders.  There is no sharp boundary between part and categorical units.  Strong recurrent connections imply that the unit is effectively deeper in the hierarchy, since the activity is more determined by other hidden units and less by the input (since the magnitude of the input connections is bounded and thereby fixed).

Properties of categorical units: encoder poorly aligned with decoder; positive recurrent connections better aligned with the decoder and slightly less categorical (but keep in mind that stroke units tend to have very weak positive recurrent connections); negative recurrent connections less aligned with the decoder and more categorical; strong, disproportionately negative recurrent connections; strong connections through the classification dictionary; units don't activate solely in response to the encoder (first iteration); active at the end of processing no more than 10\% of the time; activity refines dramatically over time.  Categorical units thus perform something closer to conventional pooling than part-units: they take their input primarily from part-units and other categorical-units, summing activity from similar units and subtracting activity from dissimilar units.  

Higher-level features (categorical units) are actually composed from low-level features (part units); they are not simply discriminative parts or templates
  - show evolution of reconstructed dictionaries
  - show evolution in discrimination accuracy with layer (mutual information between a single unit and the class?)

Importance of discriminative training already demonstrated for implicit encoders, but mostly reflect improvement in features, rather than feature combination.  Show that performance is much worse if we just train a classifier on top of a fixed unsupervised-pretrained LISTA encoder; categorical units are essential for good classification.  
 
We don't want or expect the network to develop a purely feedforward (hierarchical) architecture, since then there was no purpose for the recurrence to begin with, and the extra connections are purely extraneous.  The fact that encoder inputs to categorical units don't disappear even given strong L1 weight regularization suggests that the direct input to these units remains important, even if it isn't a template and is difficult to interpret.


The emergence of hierarchy within a recurrent network is consistent with the observation of simple and complex cells in primary visual cortex.  
Functional connectivity between simple cells and complex cells in cat striate cortex - Jose-Manuel Alonso and Luis M. Martinez (1998)

Bounding the magnitude of the encoder rows is necessary for good performance.  Why?!?  Probably forces the network to use the full ISTA stack, rather than just the last layer.  Yann says this is one of Geoff's standard tricks; citation?

L1 regularization on a hierarchical network of rectified linear units is equivalent to $\sum_{ij} \frac{\partial h_i}{\partial x_j} \cdot x_j$ where $i$ runs over the hidden units and $j$ runs over the inputs.  
\begin{align*}
z_j &= \Theta\left(\sum_i W_{ji} \cdot y_i \right) \\
\frac{d z_j}{d x_k} &= \left( \sum_i W_{ji} \cdot y_i > 0 \right) \cdot \sum_i W_{ji} \cdot \frac{d y_i}{d x_k} 
\end{align*}
This applies until the last layer, at which point $\frac{d x_i}{d x_k} = \delta_{ik}$.  Taking $L = \sum_k \frac{d z_j}{d x_k} \cdot x_k$ and pushing the sum over $k$ all the way back to the final $\delta$, we can then show that $L$ is identical to $z_j$.  THIS ONLY WORKS if we add L1 regularizers into the loss function complementary to the biases of the encoder.

Our LISTA encoder can be understood as a learned approximation to a second-order minimization of the loss (L2 reconstruction plus L1 sparsifying).  The learned matrices are effectively the optimal matrices from ISTA multiplied by a (fixed) approximation to the inverse Hessian.  In general, encoders that are complementary to a given decoder can sensibly be of the form of (first- or second-order) gradient descent on the decoder's loss function. 

Gradient magnitudes (presently approximated with backprop messages) seem to drop off by a factor of 20 between the last and first layer with 11 ista iterations in an untrained network.  This decay would obviously be eliminated if the reconstruction loss were applied to each layer, but that strategy is incommensurable with the concept of deep networks (although, similar to deep convex networks, each layer could be trained to classify through an independent logistic regressor).  Linked weights/recurrent dynamics allow the large gradients of the early layers to bootstrap the training of the early layers; if the weights are small, then hidden units activities and inputs are small even deep in the network, which renders the network similar to a shallow network.

The small set of categorical units is apparently *not* induced by the need to produce large outputs to satisfy the cross-entropy loss, in conjunction with the L1 regularization of the hidden units.  Since the sparse code is subject to L2 normalization before passing through the logistic classifier, scaling up the maximum unit is equivalent to scaling down the other units.  Moreover, cross-entropy loss is dependent upon the L1 norm within the group selected by the classification matrix, which has been normalized to have constant L2 norm.  This sum is maximized when all units have equal activity, since n/sqrt(n) = sqrt(n) > 1, rather than by a single large unit.  Moreover, the L2 magnitude of each classification matrix column is bounded; multiplying the sparse code by a row of the classification matrix is like computing and L2 norm squared, rather than an L1 norm (consider the case where the two vectors are equal).  Since both vectors are L2 normalized, the magnitude of their product is identical (and maximized) when they are aligned, regardless of how sparse they are.  The direct L1 norm on the sparse coding units then probably induces the sparse code to be as sparse as possible; this is not achieved by a disproportionately large categorical unit, which obviously hurts the unnormalized L1 loss.  But why bother making the categorical units so large, when the effect of the part units on the classification can be eliminated by just setting their connections in the classification dictionary to zero (which is already basically done).  These considerations suggest that group sparsity alone is not sufficient to induce categorical units.

Consider the possibility that large categorical-unit outputs are necessary to classify effectively based upon part units.  The magnitude of part unit activations is controlled by their reconstruction of the input; once the input is fully reconstructed, part units are not subject to further activation.  Categorical units, in contrast, do not have such an obvious indicator of semantic saturation, although one could be built by effectively including an extra unit in the input with constant activation 1, which needs to be reconstructed from the categorical units; the resulting constant input could be incorporated into the encoding matrix bias, and the resulting contribution to the explaining-away matrix would take the form of negative connections between categorical units, which are in fact observed.  Note that the output of the categorical units does not by any means grow without bound.  Rather, it seems to consistently be less than 0.5.  Correspondingly, the decoding matrix column norms remain at or near 1, so these units actually make a substantial contribution to the reconstruction, and the network does use something like a template-plus-correction structure as suggested by Eero's student.  

It's surprising that categorical units have significant decoding column magnitude.  The decoding column magnitudes are only bounded above, so when discriminative training is applied, it would be possible for a subset of units to learn to produce the correct classification without affecting the reconstruction; the remaining plurality of part-units could then adjust to fully reconstruct the input.  The fact that categorical units maintain large decoder columns despite the disproportionately large activations they achieve indicates that they are actually an important part of the reconstruction process.  I suspect, though that the part-units adjust to the contribution of the decoder, and the magnitude of the categorical-unit contribution to the reconstruction is not tightly regulated.  The categorical units need to be very active to satisfy the discriminative classification loss, and the network doesn't want to waste this potential contribution to the reconstruction, but they don't need to be precise.

How do the categorical units compute their activation?  The alignment between encoder and decoder, and the match between explaining-away weights and the dot product of the associated decoder columns, indicate that part units use ISTA even after discriminative training, and thus follow the gradient of the sum of the L2 reconstruction loss and the L1 sparsifying loss.  Categorical units, in contrast, do not have well-structured encoders, and have explaining-away connections that poorly match the dot product of the corresponding decoding columns, and so are probably not following the gradient of the L2 reconstruction loss, even though they make a substantial contribution to the reconstruction.  Their strongest connections seem to be negative links to other categorical units, which suggests they're implementing winner-take-all dynamics.  

Part-units don't seem to receive disproportionately strong connections from categorial units (which could be used to silence parts incompatible with the chosen template) (BUT keep in mind that categorical units have higher activities, so part-units could be silenced even by connections from categorical units that aren't substantially larger than those to other part-units), suggesting that the space of perturbations available from a template is not restricted by the choice of the template.  However, the template chosen is almost certainly a function of the initial part-unit activations.  Based upon the observed connections to categorical units, I would think that categorical units are activated by a very unselective encoder, and then refined by inhibition from part-units, but then categorical units should be disproportionately active on the first ISTA iteration, whereas the opposite is true.

QUESTIONABLE: The average final magnitude of categorical units is strongly correlated with their recurrent connection magnitude when they turn on.  Does this indicate that it's a constant, independent of the input, once the categorical unit is selected?  Consider looking at the variance of unit activity, given that the unit is on.  

If the wrong template is turned on, it will explain the input relatively poorly.  Assuming that the part-units continue to obey ISTA-type dynamics, a relatively large set of part-units will need to be significantly activated to explain the residual input.  Moreover, the residual input will presumably look very unlike the class associated with the active template.  As a result, the active part units will activate categorical units associated with the true class, and the spuriously activated categorical unit will not be able to sustain activity, since few of the associated part units will be active.  Discrepancies between the template and the input explained by category-appropriate part units correspond to the data manifold, centered on the template, and only serve to further activate the categorical-unit/template.  Discrepancies between the template and the input explained by category-incompatible part units correspond to errors orthogonal to the data manifold, and serve to suppress the categorical-unit/template, either directly or via activation of incompatible categorical units.  Given that a categorical unit is on, show that connections to it from the active part units are almost all positive.

The part-units that connect to a categorical-unit should not primarily be part-units that align perfectly with the categorical-unit's template; these part-units should only be weakly co-activated with the categorical unit, since the categorical unit itself can account for these components of the input.  Rather that part-units that connect most strongly to a categorical unit should be those that are slightly misaligned with the template, and so can deform the template in category-consistent ways.  (This account is consistent with the mean-zero nature of the part-based decoders, versus the positive-mean categorical-unit decoders)  The encoder for a categorical-unit needs to be diffuse, since assuming it regulates the overall activity of the categorical-unit, it must respond to all manifestations of the category, rather than just the optimal one.  HOWEVER, the most categorical units seem to have extremely weak self-connections (aside from the implicit identity component of the explaining-away matrix); that is, they integrate the evidence they receive, rather than looking at the difference between their input and their prediction.

This is different from classification based upon a sparse coding decomposition, since it projects into the space of deviations from a template, which is not the same as the space of template-free parts.  For instance, a 4 can easily be constructed using the parts of a 9, making it difficult to distinguish the two.  However, starting from a 9 template, the parts required to break the top loop or add an extension to the side are outside the data manifold of the 9 class, and so will tend to change the active template.  It is related to but distinct from group sparsity (and Simoncelli's template-plus-perturbation approach) in that the encoder does not perform any explicit energy minimization, and the group structure is based upon the need to produce very large outputs for the classifier.  Just as the units fragment into part- and categorical- units, the sparsification performed by the L1 regularizer separates into the creation of a sparse set of classifiers, and then a separate sparse set of perturbations of the classifier templates.  The categorical units must be large to saturate the softmax, and the network doesn't waste their ability to reconstruct the input.  However, the categorical unit activations must be calculated based upon the very categorical and part units that induce the final reconstruction and classification.

Only a subset of the perturbations are available for each template because of the explaining-away connections from part units to categorical units.  Part units with strong negative connections to a categorical unit cannot be used to perturb the template of that categorical unit.

An ISTA-like algorithm, where activity of the parts effectively decreases the input (e.g. via the categorical units) converges so long as the change in the input is small enough and the templates contribution induced by a part is weak.  Effectively, the categorical unit's template output is added to the part-unit's output (although it is delayed by an iteration; the steady state is identical), but the encoder input to the part-unit remains unchanged, so it minimizes the reconstruction error of the residual including the categorical units, assuming that it will not make a contribution based upon the categorical-unit/template.  If the contribution of the categorical-unit induced by the part-unit is smaller than the part contribution, the loss should still go down.  In order for the loss to be a Lyapunov function, we only need that the step in activation-space have a positive dot-product with the gradient; it doesn't need to follow the gradient exactly.

The most categorical units integrate their inputs over time, as is evidenced by their disproportionately large diagonal elements in the explaining-away matrix.

How does the classification output evolve over time?  Does the network ever change its mind?  The part-units seem to perform something close to ISTA, with the caveat that they only need explain the portion of the input not explained by the categorical units.  As a result, part-units that are essential for explaining the full input may well turn off as the network evolves, and categorical units come to explain much of the input.  In particular, part-units that initially activated the categorical units may not remain on if they align well with the decoders of the categorical units.  If a categorical unit is chosen poorly, its decoding column will not align with the input, and part-units will be activated to explain the negative image of the categorical-unit's template.  This might be an effective sign that a categorical-unit is poorly chosen.  


Restrictions on the encoder row magnitude may make sense even independent of discriminative fine-tuning.  ISTA depends upon a small step size.  As I've observed when I've tried to initialize the matrices based upon elements of the dataset, if the step size is large, it's easy to get oscillating dynamics, where the hidden units are over-activated by the encoding matrix, and then silence each other on the next iteration.  With hundreds of elements, it is likely that many units will be activated in response to each part of the input.  If the resulting total input explained is greater than the actual input, then all of these units will be inhibited on the next iteration.  As the number of units increases, it seems likely that the ISTA step size must decrease.  This might also hold for the classification dictionary.


Inhibition between categorical units is akin to explaining-away between part units; the fact that there is a classification needs to be explained, so the more the magnitude of the classification output is accounted for by one unit, the less needs to be explained by others.  The diagonal of the explaining away matrix for part units would probably go more negative if the optimal output were bounded (e.g. cross entropy loss with an optimal distribution other than 1,0,0,...)


Merely having perturbation features available, rather than the standard sparse features, allows the generalization of these perturbations to multiple templates.  In tangent-propagation (related to contractive autoencoders), the network learns that certain transformations of the input have no impact on the classification.  Similarly, if a perturbation/part unit makes no contribution to the classification (which is generally the case), then combined with the ISTA-like dynamics of the part units, that perturbation can be applied to any template.  The part/perturbation untis do influence the template selected, but not the classification associated with that template.





Future directions:

Consider adding to the encoder a term corresponding to the gradient of a weighted L2 norm, corresponding to group sparsity.  The most obvious thing lacking in the current linear decoder is clustering amongst groups of related units.  Note that the structure of the sparsity is completely enforced by the encoder; the decoder is agnostic to sparsity considerations.  It might be easier to introduce structured sparsity to the encoder than the loss function, since the parameters of the structured sparsity must themselves be trained, and will become degenerate if unconstrained.
Obvious desirable additions to the decoder include translation and rotation of the dictionary elements (although these make the network non-permutation invariant).  However, this seems likely to require an encoder that is similarly able to translate or rotate its filters.

Can we formulate an encoder structure that captures the operation of FISTA (i.e., momentum)?  It seems like this would only require adding in an explaining-away-type matrix to layer n-2, rather than just n-1; the first explaining matrix would need to be initialized so that the sum of the n-1 and n-2 contributions effectively implements momentum. 

After the first layer, consider using the reconstruction pooling loss with an ISTA encoder for all layers.  This does not directly apply group sparsity, since we don't take the square root of the sum of squares at any point, but does capture the notion that we want to identify the active low-dimensional subspaces, without caring much about the exact position within a subspace

Since categorical units produce outputs that are much larger than part units, any part unit in a group with an active categorical unit will effectively not be subject to L1 sparsification.  This is basically what we want; the network is penalized for turning on each new low-dimensional subspace, but not for the position within the subspace.  

Class-based maximum-log-likelihood training is in some ways similar to group sparsity.  Rather than applying a constant L1 sparsifying regularizer to the square-root-of-sum-of-squares, a variably-weighted L1 regularizer is applied to the logistically transformed input.  However, like group sparsity, the elements of one group (connected to the correct class output) are encouraged to be large, while the elements of the other groups are encouraged to be small.  The similarity is not perfect, in that the off-groups are not driven to be exactly zero; only much smaller than the on-group.  The penalty for the largest group is basically linear.  This suggests that similar results may be achieved with group sparsity rather than discriminative MAP/cross-entropy training.  What happens if we apply group sparsity with ten groups on MNIST?!?  If the grouping is fixed, it must be applied from the very beginning.  Categorial units generally produce very large outputs, and may be dependent upon the strong positive drive provided by the cross-entropy loss.  However, this might still occur if group sparsity is applied after global L2 normalization, since large absolute activity of one group reduces the normalized activity of the other groups.  Remember that L1 sparsification is applied to the categorical units even with the cross-entropy loss, and L2 group sparsity is no harsher than the individual L1 term.  We just want to exchange a group term that encourages a single trainable group to be on with one that encourages a sparse set of fixed groups to be on.  Note that after group-L2 normalization, L1 loss is minimized by making the inputs to one of the groups very large.  NOT TRUE - see above discussion on normalizations.


The sum of these losses is minimized by a small number of large outputs, which given the bounds on the encoding matrix, can only be achieved by large-magnitude rows in the explaining-away matrix.  It's a little surprising that this isn't achieved by reducing the magnitude of units that don't contribute to the desired classification, but this might be due to the nature of the logistic classifier; note that the classification matrix itself becomes sparse.  In contrast, the L2 loss is minimized when all units in a group have equal activity, and so will not induce categorical units.  Part of the issue is probably that L1 sparsification is done before L2 normalization.  




Citations:

Learning Efficient Structured Sparse Models - Sprechmann, Bronstein, Sapiro (2012) ICML - Very similar, but does not backpropagate the classification loss through the encoder; just directly hooks the encoder to the decoder and trains as a (structured) sparse autoencoder.  A separate dictionary is trained for each digit class, and classification is done on the basis of minimum reconstruction-plus-sparsifying loss.  Performance is poor; > 2\%.  

ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning - Le, Karpenko,Ngiam, \& Ng - uses pure linear encoder, rather than LISTA stack

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 34, NO. 4, APRIL 2012 791
Task-Driven Dictionary Learning Julien Mairal, Francis Bach, and Jean Ponce
Get great MNIST performance, but use shifted images in the dataset.  Minimization of sparse coding objective is direct/implicit.

J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman, “Supervised Dictionary Learning,” Proc. Advances in Neural Information Processing Systems, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, eds., vol. 21, pp. 1033-1040, 2009.

Basis pursuit (denoising): S. Chen, D. Donoho, and M. Saunders, “Atomic decomposition by basis pursuit,” SIAM J. Scientific Computing, vol. 20, no. 1, pp. 33–61, 1999.
Lasso: R. Tibshirani, “Regression shrinkage and selection via the LASSO,” J. Royal Stat. Society: Series B, vol. 58, no. 1, pp. 267–288, 1996.

Ekanadham, Tranchina, \& Simoncelli (2011). Recovery of Sparse Translation-Invariant Signals With Continuous Basis Pursuit  - actually seems to be equivalent to group sparse coding with additional (convex?) constraints on the groups - my networks seem to be using a template-plus-perturbation formulation similar to the Taylor approximation in this paper



Deep Sparse Rectifier Neural Networks - Xavier Glorot, Antoine Bordes and Yoshua Bengio
Find that softplus works better than rectified-linear; don't find significant advantage to pretraining; don't link weights between layers; don't feed input into all layers

Deep convex networks
Do feed input into all layers

Yoshua's deep learning review paper

K. Gregor and Y. LeCun, “Learning fast approximations of sparse coding,” in ICML, 2010, pp. 399–406. - LISTA paper from Karol
Gregor, Szlam, LeCun - Structured sparse coding via lateral inhibition


Standard citation for sparse autoencoders? - I. Goodfellow, Q. Le, A. Saxe, H. Lee, and A. Y. Ng, “Measuring invariances in deep networks,” in In NIPS, 2009, pp. 646–654.

PSD paper from Koray

Gated softmax classification - R Memisevic, C Zach, G Hinton - equivalent to soft-plus; smooth version of rectified-linear

Hinton paper recovering softplus (smoothed rectified linear) from RBM using replicated units with different biases - Nair and Hinton

Differentiable Sparse Coding - Bradley and Bagnell - Find the gradient of true sparse coding output with respect to the parameters and use it for discriminative gradient descent training - requires inversion of large matrix - MNIST error 1.3\%


R. Raina, A. Battle, H. Lee, B. Packer, and A.Y. Ng, “Self-Taught Learning: Transfer Learning from Unlabeled Data,” Proc. Int’l Conf. Machine Learning, 2007.

Sparse coding (or at least linear reconstruction with L2 reconstruction loss) seems to be the basis of A Large-Scale Model of the Functioning Brain - Chris Eliasmith et al. (2012), published in Science


Contractive Auto-Encoders: Explicit Invariance During Feature Extraction - Rifai, ..., Bengio (2011) - provide performance data on MNIST (1.14\%) and CIFAR-bw (47.86\%) - L1 loss on rectified linear stack minimizes something similar to the Frobenius norm of the Jacobian



Babel site visit:

Motivation:
Deep networks are powerful, but depend upon heuristic, computationally intensive pretraining
Want a simple, direct way to pretrain a deep encoder

Sparse coding is powerful, but depends upon slow iterative procedures for encoding
Want a simple, direct encoder for sparse coding

%Deep belief networks provide an effective heuristic for training deep networks, but only weak bounds on log likelihood can be optimized efficiently
% Importance of discriminative training already demonstrated for implicit encoders, but mostly reflect improvement in features, rather than feature combination

Approach:
Unroll the ISTA sparse coding algorithm into a deep feedforward network with rectified-linear nonlinearities
Weights are tied between layers
  - Convolutional architecture encodes translational invariance
  - Tied weights between layers encodes depth invariance
  - Each unit has a consistent meaning between layers, but accuracy evolves across layers (accuracy defined by loss function)
  - Equivalent to extended temporal processing in a fixed one-layer network
Decoding dictionary allows hidden units to be interpretted
Pretrain as a sparse autoencoder, then add in a cross-entropy classification loss and backpropagate

PRELIMINARY RESULTS
Preliminary experiments with small networks yield good performance on MNIST (200 units, ~1.2; 400 units, ~1.1 \%)
In addition to ``part'' units formed during unsupervised pretraining, discriminative training  creates mid-level features and classifier units
FIGURE: evolution of reconstructed dictionary, classification dictionary

Future steps:
Increase size
Convolutionalize